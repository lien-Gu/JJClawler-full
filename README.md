# 晋江文学城爬虫后端项目设计文档

## 1. 项目概述

### 1.1 项目背景
晋江文学城是国内知名的网络文学平台，拥有大量的小说作品和活跃的读者群体。本项目旨在开发一个数据爬虫后端服务，用于采集平台上的榜单数据和小说信息，为作者和读者提供数据分析基础。

### 1.2 项目目标
- 实现自动化的榜单数据采集
- 提供RESTful API接口供前端调用
- 支持定时任务和手动触发
- 确保系统稳定性和数据准确性
- 快速完成MVP验证项目可行性

### 1.3 项目范围
- **包含**：榜单爬取、小说信息爬取、数据存储、API服务、定时调度
- **不包含**：前端界面、数据分析功能、用户系统

## 2. 需求分析

### 2.1 功能需求

#### 2.1.1 数据爬取需求
| 功能模块 | 需求描述 | 优先级 |
|---------|---------|--------|
| 夹子榜单爬取 | 每小时更新一次夹子榜单数据 | P0 |
| 分类榜单爬取 | 按配置频率爬取各分类榜单（言情、纯爱、衍生等） | P0 |
| 小说详情爬取 | 爬取榜单中小说的详细信息 | P0 |
| 增量更新 | 仅爬取新增或变化的数据 | P2 |

#### 2.1.2 API接口需求
| 接口类型 | 功能描述 | 优先级 |
|---------|---------|--------|
| 榜单查询 | 查询历史榜单数据、最新榜单 | P0 |
| 小说查询 | 查询小说列表、小说详情 | P0 |
| 任务管理 | 查看爬取任务状态、手动触发爬取 | P1 |
| 统计信息 | 系统运行统计、数据统计 | P1 |

### 2.2 非功能需求

| 需求类型 | 具体要求 | 备注 |
|---------|---------|------|
| 性能要求 | 支持每小时处理1000+请求 | 2C4G服务器限制 |
| 可靠性 | 系统可用性>95%，支持断点续爬 | - |
| 安全性 | 遵守robots协议，控制请求频率 | 避免被封禁 |
| 可维护性 | 代码简洁易读，模块化设计 | 便于后续扩展 |
| 部署要求 | 支持Docker容器化部署 | Linux环境 |

### 2.3 数据需求

#### 2.3.1 爬取数据结构

参考data/example中的文件

## 3. 系统架构设计

### 3.1 技术选型

| 技术栈 | 选择             | 选择理由 |
|--------|----------------|----------|
| 编程语言 | Python 3.10+   | 爬虫生态成熟，开发效率高 |
| Web框架 | FastAPI        | 高性能，自动生成API文档 |
| HTTP库 | Requests       | 简单易用，稳定可靠 |
| 数据库 | SQLite → MySQL | 开发用SQLite，生产可迁移MySQL |
| ORM | SQLAlchemy     | 功能强大，支持多种数据库 |
| 任务调度 | APScheduler    | 轻量级，满足需求 |
| 容器化 | Docker         | 便于部署和环境一致性 |

### 3.2 系统架构图

```
┌─────────────────────────────────────────────────────┐
│                    客户端层                          │
│            (Web前端 / 移动端 / API调用方)            │
└─────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────┐
│                    API网关层                         │
│                 FastAPI Router                       │
│          /rankings  /novels  /tasks  /stats         │
└─────────────────────────────────────────────────────┘
                            │
        ┌───────────────────┼───────────────────┐
        ▼                   ▼                   ▼
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│  业务逻辑层  │    │  任务调度层  │    │  爬虫引擎层  │
│  API Service │    │  Scheduler   │    │   Crawler    │
└─────────────┘    └─────────────┘    └─────────────┘
        │                   │                   │
        └───────────────────┼───────────────────┘
                            ▼
┌─────────────────────────────────────────────────────┐
│                    数据访问层                        │
│                  SQLAlchemy ORM                      │
└─────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────┐
│                    数据存储层                        │
│                  SQLite / MySQL                      │
└─────────────────────────────────────────────────────┘
```

### 3.3 模块设计

#### 3.3.1 核心模块

| 模块名称 | 职责 | 主要组件 |
|---------|------|----------|
| main.py | 应用入口 | FastAPI实例、生命周期管理 |
| config.py | 配置管理 | 环境变量、URLs配置加载 |
| models.py | 数据模型 | ORM模型定义、数据库连接 |
| crawler.py | 爬虫核心 | HTTP请求、重试机制、数据采集 |
| scheduler.py | 任务调度 | 定时任务、任务管理 |
| api/endpoints.py | API接口 | RESTful接口实现 |
| utils/parser.py | 数据解析 | 响应解析、数据转换 |

#### 3.3.2 数据流程

```
用户请求 → API接口 → 业务逻辑处理 → 数据库查询 → 返回结果
    ↓
定时触发 → 调度器 → 爬虫任务 → 数据解析 → 存储数据库
```

## 4. 详细设计

### 4.1 数据库设计

#### 4.1.1 数据表结构

**ranking_data（榜单数据表）**
| 字段名 | 类型 | 说明 | 约束 |
|--------|------|------|------|
| id | INTEGER | 主键 | PRIMARY KEY |
| page_type | VARCHAR(50) | 页面类型 | INDEX |
| channel | VARCHAR(50) | 频道标识 | INDEX |
| zh_name | VARCHAR(50) | 中文名称 | - |
| crawl_time | DATETIME | 爬取时间 | INDEX |
| data | JSON | 完整榜单数据 | - |
| created_at | DATETIME | 创建时间 | DEFAULT NOW |

**novels（小说信息表）**
| 字段名 | 类型 | 说明 | 约束 |
|--------|------|------|------|
| id | INTEGER | 主键 | PRIMARY KEY |
| novel_id | VARCHAR(50) | 小说ID | UNIQUE INDEX |
| title | VARCHAR(200) | 书名 | - |
| author | VARCHAR(100) | 作者 | INDEX |
| tags | JSON | 标签列表 | - |
| data | JSON | 完整数据 | - |
| last_updated | DATETIME | 最后更新 | - |
| created_at | DATETIME | 创建时间 | DEFAULT NOW |

**crawl_tasks（爬取任务表）**
| 字段名 | 类型 | 说明 | 约束 |
|--------|------|------|------|
| id | INTEGER | 主键 | PRIMARY KEY |
| task_type | VARCHAR(50) | 任务类型 | - |
| channel | VARCHAR(50) | 频道 | - |
| status | VARCHAR(20) | 状态 | - |
| start_time | DATETIME | 开始时间 | - |
| end_time | DATETIME | 结束时间 | - |
| error_message | TEXT | 错误信息 | - |
| items_crawled | INTEGER | 爬取条目数 | DEFAULT 0 |
| created_at | DATETIME | 创建时间 | DEFAULT NOW |

### 4.2 API接口设计

#### 4.2.1 接口清单

| 方法 | 路径 | 功能 | 请求参数 | 响应格式 |
|------|------|------|----------|----------|
| GET | /health | 健康检查 | - | {status, timestamp} |
| GET | /api/v1/rankings | 获取榜单列表 | page_type, channel, limit, offset | {total, data[]} |
| GET | /api/v1/rankings/latest | 获取最新榜单 | - | {count, data[]} |
| GET | /api/v1/novels | 获取小说列表 | author, status, limit, offset | {total, data[]} |
| GET | /api/v1/novels/{novel_id} | 获取小说详情 | novel_id | {novel_data} |
| GET | /api/v1/tasks | 获取任务列表 | status, limit | {count, data[]} |
| GET | /api/v1/jobs | 获取调度任务 | - | {jobs[]} |
| POST | /api/v1/crawl/jiazi | 触发夹子爬取 | - | {message} |
| POST | /api/v1/crawl/page | 触发页面爬取 | page_type, channel | {message} |
| GET | /api/v1/stats | 获取统计信息 | - | {statistics} |

### 4.3 爬虫策略设计

#### 4.3.1 请求策略
- **User-Agent**: 模拟移动端APP请求
- **请求间隔**: 1秒（可配置）
- **超时时间**: 30秒
- **重试机制**: 失败重试3次，指数退避

#### 4.3.2 调度策略
| 页面类型 | 更新频率 | 说明 |
|------|---------|------|
| 夹子榜单 | 1小时 | 热度榜单，更新频繁 |
| 其他榜单 | 24小时 | 日更新即可 |

#### 4.3.3 异常处理
- 网络异常：记录日志，等待重试
- 解析异常：保存原始数据，标记异常
- 频率限制：延长请求间隔，暂停任务

## 5. 部署方案

### 5.1 开发环境（macOS）
```bash
# 环境要求
- Python 3.10+
- SQLite3
- Git

# 部署步骤
1. 克隆代码库
2. 创建虚拟环境
3. 安装依赖
4. 初始化数据库
5. 启动服务
```

### 5.2 生产环境（Linux + Docker）

#### 5.2.1 Docker镜像构建
```dockerfile
FROM python:3.13-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0"]
```

#### 5.2.2 部署架构
```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   Nginx     │────▶│   Docker    │────▶│   sqlite     │
│  (可选)     │     │  Container  │     │  (可选)     │
└─────────────┘     └─────────────┘     └─────────────┘
```

### 5.3 监控方案
- **日志监控**: 使用日志文件记录关键操作
- **健康检查**: 定期调用/health接口
- **数据监控**: 统计爬取成功率、数据量
- **告警机制**: 失败率超过阈值时告警

## 6. 测试方案

### 6.1 测试策略

| 测试类型 | 测试内容 | 测试方法 |
|---------|---------|----------|
| 单元测试 | 数据解析、工具函数 | pytest |
| 接口测试 | API接口功能 | Postman/pytest |
| 集成测试 | 爬虫流程、数据存储 | 手动测试 |
| 压力测试 | 并发请求处理 | locust |

### 6.2 测试用例示例

#### 6.2.1 爬虫功能测试
- 测试正常爬取流程
- 测试网络异常处理
- 测试数据解析异常
- 测试重复数据处理

#### 6.2.2 API接口测试
- 测试参数校验
- 测试分页功能
- 测试错误响应
- 测试并发请求

## 7. 风险评估

| 风险类型 | 风险描述 | 应对措施 |
|---------|---------|----------|
| 技术风险 | API接口变更 | 灵活的解析器设计，快速适配 |
| 技术风险 | 反爬虫机制 | 控制频率，模拟正常请求 |
| 性能风险 | 数据量增长 | 数据库优化，定期清理 |
| 运维风险 | 服务器资源限制 | 监控资源使用，优化代码 |
| 法律风险 | 数据使用合规 | 仅用于研究分析，遵守协议 |

## 8. 项目计划

### 8.1 开发阶段（2周）

**第一周：基础框架搭建**
- Day 1-2: 项目结构搭建，环境配置
- Day 3-4: 数据模型设计，数据库初始化
- Day 5-7: 爬虫核心功能实现

**第二周：功能完善**
- Day 8-9: API接口开发
- Day 10-11: 调度系统实现
- Day 12-13: 测试和Bug修复
- Day 14: 文档编写，部署准备

### 8.2 里程碑
1. **M1**: 完成基础爬虫功能（第3天）
2. **M2**: 完成API接口（第9天）
3. **M3**: 完成调度系统（第11天）
4. **M4**: 项目上线（第14天）

## 9. 后续优化建议

### 9.1 短期优化（1个月内）
- 实现增量更新机制
- 添加数据去重逻辑
- 优化数据库查询性能
- 完善错误处理机制

### 9.2 中期优化（3个月内）
- 引入Redis缓存
- 实现分布式爬虫
- 添加数据分析功能
- 开发前端界面

### 9.3 长期规划
- 支持更多数据源
- 机器学习预测分析
- 实时数据推送
- 用户个性化服务

## 10. 附录

### 10.1 参考资料
- FastAPI官方文档: https://fastapi.tiangolo.com
- SQLAlchemy文档: https://www.sqlalchemy.org
- APScheduler文档: https://apscheduler.readthedocs.io

### 10.2 相关规范
- RESTful API设计规范
- Python PEP8编码规范
- Git提交信息规范

### 10.3 联系方式
- 项目负责人：[Lien Gu]
- 技术支持：[guliyu0216@163.com]
- 项目仓库：[Git地址]