# 晋江文学城爬虫后端项目设计文档

## 1. 项目概述

### 1.1 项目背景
晋江文学城是国内知名的网络文学平台，拥有大量的小说作品和活跃的读者群体。本项目旨在开发一个数据爬虫后端服务，用于采集平台上的榜单数据和小说信息，为作者和读者提供数据分析基础。

### 1.2 项目目标
- 实现自动化的榜单数据采集
- 提供RESTful API接口供前端调用
- 支持定时任务和手动触发
- 确保系统稳定性和数据准确性
- 快速完成MVP验证项目可行性

### 1.3 项目范围
- **包含**：榜单爬取、小说信息爬取、数据存储、API服务、定时调度
- **不包含**：前端界面、数据分析功能、用户系统

## 2. 需求分析

### 2.1 功能需求

#### 2.1.1 数据爬取需求
| 功能模块 | 需求描述 | 优先级 |
|---------|---------|--------|
| 夹子榜单爬取 | 每小时更新一次夹子榜单数据 | P0 |
| 分类榜单爬取 | 按配置频率爬取各分类榜单（言情、纯爱、衍生等） | P0 |
| 小说详情爬取 | 爬取榜单中小说的详细信息 | P0 |
| 增量更新 | 仅爬取新增或变化的数据 | P2 |

#### 2.1.2 API接口需求
| 接口类型 | 功能描述 | 优先级 |
|---------|---------|--------|
| 榜单查询 | 查询历史榜单数据、最新榜单 | P0 |
| 小说查询 | 查询小说列表、小说详情 | P0 |
| 任务管理 | 查看爬取任务状态、手动触发爬取 | P1 |
| 统计信息 | 系统运行统计、数据统计 | P1 |

### 2.2 非功能需求

| 需求类型 | 具体要求 | 备注 |
|---------|---------|------|
| 性能要求 | 支持每小时处理1000+请求 | 2C4G服务器限制 |
| 可靠性 | 系统可用性>95%，支持断点续爬 | - |
| 安全性 | 遵守robots协议，控制请求频率 | 避免被封禁 |
| 可维护性 | 代码简洁易读，模块化设计 | 便于后续扩展 |
| 部署要求 | 支持Docker容器化部署 | Linux环境 |

### 2.3 数据需求

#### 2.3.1 爬取数据结构

参考data/example中的文件

## 3. 系统架构设计

### 3.1 技术选型

| 技术栈 | 选择 | 选择理由 |
|--------|------|----------|
| 编程语言 | Python 3.13+ | 爬虫生态成熟，开发效率高 |
| Web框架 | FastAPI | 高性能，自动生成API文档，轻量级 |
| HTTP库 | httpx | 异步支持，性能更好 |
| 数据存储 | JSON文件 | 轻量级，便于调试和查看 |
| 任务调度 | cron + shell | 简单可靠，易于管理 |
| 依赖管理 | Poetry | 现代Python包管理 |

### 3.2 基于接口驱动的模块化架构

```
┌─────────────────────────────────────────────────────┐
│                  API接口层                           │
│              (定义所有对外接口)                       │
└─────────────────────────────────────────────────────┘
                            │
                    根据接口确定功能模块
                            ▼
┌─────────────────────────────────────────────────────┐
│                  功能模块层                          │
│   爬虫模块    数据模块    统计模块    任务模块        │
└─────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────┐
│                  工具支持层                          │
│      HTTP工具   文件工具   时间工具   日志工具       │
└─────────────────────────────────────────────────────┘
```

### 3.3 接口驱动的开发流程

#### 3.3.1 第一阶段：接口定义
1. 根据需求确定所有API接口
2. 定义接口的输入输出格式
3. 创建接口路由和基础结构

#### 3.3.2 第二阶段：功能模块拆分
根据接口需求，拆分为以下独立模块：

| 模块名称 | 负责接口 | 核心功能 | 文件位置 |
|---------|---------|---------|----------|
| 爬虫模块 | `/crawl/*` | 数据采集、解析、存储 | `modules/crawler.py` |
| 数据模块 | `/api/v1/rankings`, `/api/v1/novels` | 数据查询、过滤、分页 | `modules/data_service.py` |
| 统计模块 | `/api/v1/stats` | 数据统计、分析 | `modules/stats_service.py` |
| 任务模块 | `/api/v1/tasks`, `/api/v1/jobs` | 任务状态管理 | `modules/task_service.py` |

#### 3.3.3 第三阶段：模块独立开发
每个模块包含：
- 接口实现函数
- 数据处理逻辑  
- 单元测试
- 模块配置

### 3.4 数据存储简化设计

使用JSON文件存储，按日期和类型分类：
```
data/
├── rankings/
│   ├── 2024-01-01/
│   │   ├── jiazi.json
│   │   └── fenlei.json
├── novels/
│   └── novel_details.json
└── tasks/
    └── task_history.json
```

## 4. 详细设计

### 4.1 API接口优先设计

根据需求分析，按优先级定义接口：

#### 4.1.1 P0接口（核心功能）

**1. 健康检查接口**
```
GET /health
响应: {"status": "ok", "timestamp": "2024-01-01T00:00:00Z"}
```

**2. 榜单数据接口**
```
GET /api/v1/rankings/latest
响应: {"count": 50, "data": [ranking_items]}

GET /api/v1/rankings
参数: page_type, channel, date, limit, offset
响应: {"total": 1000, "data": [ranking_items]}
```

**3. 小说信息接口**
```
GET /api/v1/novels/{novel_id}
响应: {novel_detail}

GET /api/v1/novels
参数: author, tags, limit, offset
响应: {"total": 500, "data": [novel_items]}
```

**4. 爬虫触发接口**
```
POST /api/v1/crawl/jiazi
响应: {"message": "Task started", "task_id": "xxx"}

POST /api/v1/crawl/page
参数: {"page_type": "fenlei", "channel": "yanqing"}
响应: {"message": "Task started", "task_id": "xxx"}
```

#### 4.1.2 P1接口（管理功能）

**5. 任务管理接口**
```
GET /api/v1/tasks
参数: status, limit
响应: {"count": 10, "data": [task_items]}

GET /api/v1/tasks/{task_id}
响应: {task_detail}
```

**6. 统计信息接口**
```
GET /api/v1/stats
响应: {
  "total_novels": 1000,
  "total_rankings": 50,
  "last_crawl_time": "2024-01-01T00:00:00Z",
  "crawl_success_rate": 0.95
}
```

### 4.2 基于接口的功能模块拆分

根据上述接口设计，将功能拆分为以下独立模块：

#### 4.2.1 爬虫模块 (modules/crawler.py)
**负责接口**: `/api/v1/crawl/*`
**核心功能**:
- 夹子榜单爬取
- 分类榜单爬取  
- 数据解析和存储
- 异常处理和重试

#### 4.2.2 数据服务模块 (modules/data_service.py)
**负责接口**: `/api/v1/rankings/*`, `/api/v1/novels/*`
**核心功能**:
- 榜单数据查询和过滤
- 小说信息查询
- 数据分页处理
- JSON文件读取优化

#### 4.2.3 任务管理模块 (modules/task_service.py)
**负责接口**: `/api/v1/tasks/*`
**核心功能**:
- 任务状态跟踪
- 任务历史记录
- 异步任务处理

#### 4.2.4 统计服务模块 (modules/stats_service.py)
**负责接口**: `/api/v1/stats`
**核心功能**:
- 数据统计计算
- 系统运行状态监控
- 性能指标收集

### 4.3 爬虫策略设计

#### 4.3.1 请求策略
- **User-Agent**: 模拟移动端APP请求
- **请求间隔**: 1秒（可配置）
- **超时时间**: 30秒
- **重试机制**: 失败重试3次，指数退避

#### 4.3.2 调度策略
| 页面类型 | 更新频率 | 说明 |
|------|---------|------|
| 夹子榜单 | 1小时 | 热度榜单，更新频繁 |
| 其他榜单 | 24小时 | 日更新即可 |

#### 4.3.3 异常处理
- 网络异常：记录日志，等待重试
- 解析异常：保存原始数据，标记异常
- 频率限制：延长请求间隔，暂停任务

## 5. 部署方案

### 5.1 开发环境（macOS）
```bash
# 环境要求
- Python 3.10+
- SQLite3
- Git

# 部署步骤
1. 克隆代码库
2. 创建虚拟环境
3. 安装依赖
4. 初始化数据库
5. 启动服务
```

### 5.2 生产环境（Linux + Docker）

#### 5.2.1 Docker镜像构建
```dockerfile
FROM python:3.13-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0"]
```

#### 5.2.2 部署架构
```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   Nginx     │────▶│   Docker    │────▶│   sqlite     │
│  (可选)     │     │  Container  │     │  (可选)     │
└─────────────┘     └─────────────┘     └─────────────┘
```

### 5.3 监控方案
- **日志监控**: 使用日志文件记录关键操作
- **健康检查**: 定期调用/health接口
- **数据监控**: 统计爬取成功率、数据量
- **告警机制**: 失败率超过阈值时告警

## 6. 测试方案

### 6.1 测试策略

| 测试类型 | 测试内容 | 测试方法 |
|---------|---------|----------|
| 单元测试 | 数据解析、工具函数 | pytest |
| 接口测试 | API接口功能 | Postman/pytest |
| 集成测试 | 爬虫流程、数据存储 | 手动测试 |
| 压力测试 | 并发请求处理 | locust |

### 6.2 测试用例示例

#### 6.2.1 爬虫功能测试
- 测试正常爬取流程
- 测试网络异常处理
- 测试数据解析异常
- 测试重复数据处理

#### 6.2.2 API接口测试
- 测试参数校验
- 测试分页功能
- 测试错误响应
- 测试并发请求

## 7. 风险评估

| 风险类型 | 风险描述 | 应对措施 |
|---------|---------|----------|
| 技术风险 | API接口变更 | 灵活的解析器设计，快速适配 |
| 技术风险 | 反爬虫机制 | 控制频率，模拟正常请求 |
| 性能风险 | 数据量增长 | 数据库优化，定期清理 |
| 运维风险 | 服务器资源限制 | 监控资源使用，优化代码 |
| 法律风险 | 数据使用合规 | 仅用于研究分析，遵守协议 |

## 8. 项目计划

### 8.1 开发阶段（2周）

**第一周：基础框架搭建**
- Day 1-2: 项目结构搭建，环境配置
- Day 3-4: 数据模型设计，数据库初始化
- Day 5-7: 爬虫核心功能实现

**第二周：功能完善**
- Day 8-9: API接口开发
- Day 10-11: 调度系统实现
- Day 12-13: 测试和Bug修复
- Day 14: 文档编写，部署准备

### 8.2 里程碑
1. **M1**: 完成基础爬虫功能（第3天）
2. **M2**: 完成API接口（第9天）
3. **M3**: 完成调度系统（第11天）
4. **M4**: 项目上线（第14天）

## 9. 后续优化建议

### 9.1 短期优化（1个月内）
- 实现增量更新机制
- 添加数据去重逻辑
- 优化数据库查询性能
- 完善错误处理机制

### 9.2 中期优化（3个月内）
- 引入Redis缓存
- 实现分布式爬虫
- 添加数据分析功能
- 开发前端界面

### 9.3 长期规划
- 支持更多数据源
- 机器学习预测分析
- 实时数据推送
- 用户个性化服务

## 10. 附录

### 10.1 参考资料
- FastAPI官方文档: https://fastapi.tiangolo.com
- SQLAlchemy文档: https://www.sqlalchemy.org
- APScheduler文档: https://apscheduler.readthedocs.io

### 10.2 相关规范
- RESTful API设计规范
- Python PEP8编码规范
- Git提交信息规范

### 10.3 联系方式
- 项目负责人：[Lien Gu]
- 技术支持：[guliyu0216@163.com]
- 项目仓库：[Git地址]