# æ™‹æ±Ÿæ–‡å­¦åŸçˆ¬è™«é¡¹ç›®è¯¦ç»†è®¾è®¡æ–¹æ¡ˆ

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 é¡¹ç›®èƒŒæ™¯
æ™‹æ±Ÿæ–‡å­¦åŸçˆ¬è™«é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäºPythonçš„è½»é‡çº§æ•°æ®é‡‡é›†å’ŒAPIæœåŠ¡ç³»ç»Ÿï¼Œä¸“æ³¨äºæ™‹æ±Ÿæ–‡å­¦åŸæ¦œå•æ•°æ®çš„è‡ªåŠ¨åŒ–é‡‡é›†ã€å­˜å‚¨å’ŒæŸ¥è¯¢æœåŠ¡ã€‚

### 1.2 è®¾è®¡ç›®æ ‡
- **é«˜æ•ˆæ€§**: æä¾›å¿«é€Ÿçš„æ•°æ®é‡‡é›†å’ŒæŸ¥è¯¢æœåŠ¡
- **å¯é æ€§**: ç¡®ä¿æ•°æ®å‡†ç¡®æ€§å’Œç³»ç»Ÿç¨³å®šæ€§
- **å¯æ‰©å±•æ€§**: æ”¯æŒæ–°çš„æ•°æ®æºå’ŒåŠŸèƒ½æ‰©å±•
- **æ˜“ç»´æŠ¤æ€§**: ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ¨¡å—åŒ–è®¾è®¡
- **ç±»å‹å®‰å…¨**: å…¨é¢ä½¿ç”¨ç±»å‹æ³¨è§£å’ŒéªŒè¯

### 1.3 æŠ€æœ¯é€‰å‹åŸåˆ™
- **ç°ä»£åŒ–**: ä½¿ç”¨Python 3.12+çš„æœ€æ–°ç‰¹æ€§
- **ç±»å‹å®‰å…¨**: FastAPI + Pydantic + SQLAlchemyæä¾›å®Œæ•´çš„ç±»å‹æ£€æŸ¥
- **è½»é‡çº§**: SQLiteæ•°æ®åº“ï¼Œæ— éœ€é¢å¤–éƒ¨ç½²æˆæœ¬
- **å¼‚æ­¥ä¼˜å…ˆ**: æ”¯æŒé«˜å¹¶å‘è¯·æ±‚å¤„ç†
- **æµ‹è¯•è¦†ç›–**: å®Œæ•´çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•

## 2. ç³»ç»Ÿæ¶æ„è®¾è®¡


### 2.1 æ ¸å¿ƒæ¨¡å—ä¾èµ–å…³ç³»
```
API Layer
    â†“
Service Layer
    â†“
DAO Layer
    â†“
Model Layer
    â†“
Database Layer
```

### 2.2 æ•°æ®æµå‘
1. **çˆ¬è™«æ•°æ®æµ**: Scheduler â†’ Crawler â†’ Parser â†’ Service â†’ DAO â†’ Database
2. **APIæŸ¥è¯¢æµ**: API â†’ Service â†’ DAO â†’ Database â†’ Service â†’ API
3. **æŠ¥å‘Šç”Ÿæˆæµ**: Scheduler â†’ Service â†’ DAO â†’ Database â†’ Report Generator

## 3. æ ¸å¿ƒæ¨¡å—è®¾è®¡

### 3.1 é…ç½®æ¨¡å— (config.py)
```python
# ä½¿ç”¨Pydantic BaseSettingsè‡ªåŠ¨è¯»å–ç¯å¢ƒå˜é‡
class Settings(BaseSettings):
    # æ•°æ®åº“é…ç½®
    database_url: str = "sqlite:///./data/jjcrawler.db"
    database_echo: bool = False
    
    # APIé…ç½®
    api_title: str = "JJCrawler API"
    api_version: str = "1.0.0"
    api_debug: bool = False
    
    # çˆ¬è™«é…ç½®
    crawl_user_agent: str = "Mozilla/5.0..."
    crawl_timeout: int = 30
    crawl_retry_times: int = 3
    crawl_delay: float = 1.0
    
    # è°ƒåº¦å™¨é…ç½®
    scheduler_timezone: str = "Asia/Shanghai"
    scheduler_job_defaults: dict = {
        "coalesce": False,
        "max_instances": 1
    }
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
```

### 3.2 æ•°æ®æ¨¡å‹è®¾è®¡

#### 3.2.1 ä¹¦ç±æ¨¡å‹ (Book)
```python
class Book(Base):
    __tablename__ = "books"
    
    # ä¸»é”®å’ŒåŸºæœ¬ä¿¡æ¯
    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    novel_id: Mapped[int] = mapped_column(Integer, unique=True, index=True)
    title: Mapped[str] = mapped_column(String(200), index=True)
    author: Mapped[str] = mapped_column(String(100), index=True)
    
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.now)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.now, onupdate=datetime.now)
    
    # å…³ç³»
    snapshots: Mapped[List["BookSnapshot"]] = relationship(back_populates="book")
```

#### 3.2.2 ä¹¦ç±å¿«ç…§æ¨¡å‹ (BookSnapshot)
```python
class BookSnapshot(Base):
    __tablename__ = "book_snapshots"
    
    # ä¸»é”®
    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    book_id: Mapped[int] = mapped_column(Integer)
    
    # ç»Ÿè®¡æ•°æ®
    favorites: Mapped[int] = mapped_column(Integer, default=0)
    clicks: Mapped[int] = mapped_column(Integer, default=0)
    comments: Mapped[int] = mapped_column(Integer, default=0)
    # æ—¶é—´æˆ³
    snapshot_time: Mapped[datetime] = mapped_column(DateTime, default=datetime.now, index=True)
    
    # å…³ç³»
    book: Mapped["Book"] = relationship(back_populates="snapshots")
```

### 3.3 APIå±‚è®¾è®¡

**APIæ¨¡å—æ ¸å¿ƒåŠŸèƒ½ï¼š**
- ğŸ“š **ä¹¦ç±ä¿¡æ¯æ¥å£** (books.py)ï¼šä¹¦ç±æŸ¥è¯¢ã€è¯¦æƒ…ã€è¶‹åŠ¿åˆ†æ
- ğŸ“Š **æ¦œå•æ•°æ®æ¥å£** (rankings.py)ï¼šæ¦œå•æŸ¥è¯¢ã€å†å²æ•°æ®ã€æ’åç»Ÿè®¡
- ğŸ•·ï¸ **çˆ¬è™«ç®¡ç†æ¥å£** (crawl.py)ï¼šæ‰‹åŠ¨è§¦å‘çˆ¬å–ã€ä»»åŠ¡ç®¡ç†ã€çŠ¶æ€ç›‘æ§
- ğŸ’¾ **æ•°æ®ä¼ è¾“å¯¹è±¡** (DTOs)ï¼šè¯·æ±‚/å“åº”æ¨¡å‹å®šä¹‰ï¼Œç¡®ä¿ç±»å‹å®‰å…¨

**è®¾è®¡åŸåˆ™ï¼š**
- **RESTfulè®¾è®¡**ï¼šéµå¾ªRESTåŸåˆ™ï¼Œè¯­ä¹‰åŒ–URLè®¾è®¡
- **ç±»å‹å®‰å…¨**ï¼šä½¿ç”¨Pydanticæ¨¡å‹ç¡®ä¿æ•°æ®éªŒè¯
- **é”™è¯¯å¤„ç†**ï¼šç»Ÿä¸€çš„å¼‚å¸¸å¤„ç†å’Œé”™è¯¯å“åº”æ ¼å¼
- **æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆ**ï¼šFastAPIè‡ªåŠ¨ç”ŸæˆOpenAPIæ–‡æ¡£
- **åˆ†é¡µæ”¯æŒ**ï¼šå¤§æ•°æ®é›†è‡ªåŠ¨åˆ†é¡µå¤„ç†
- **æƒé™æ§åˆ¶**ï¼šé¢„ç•™æƒé™éªŒè¯æ‰©å±•ç‚¹

#### 3.3.1 ä¹¦ç±API (books.py)
```python
@router.get("/", response_model=List[BookResponse])
async def get_books(
    page: int = Query(1, ge=1),
    size: int = Query(20, ge=1, le=100),
    tag: Optional[str] = None,
    status: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """è·å–ä¹¦ç±åˆ—è¡¨"""
    pass

@router.get("/{novel_id}", response_model=BookDetailResponse)
async def get_book_detail(
    novel_id: int,
    db: Session = Depends(get_db)
):
    """è·å–ä¹¦ç±è¯¦æƒ…"""
    pass

@router.get("/{novel_id}/trend", response_model=BookTrendResponse)
async def get_book_trend(
    novel_id: int,
    days: int = Query(7, ge=1, le=365),
    db: Session = Depends(get_db)
):
    """è·å–ä¹¦ç±è¶‹åŠ¿æ•°æ®"""
    pass

@router.get("/search", response_model=List[BookResponse])
async def search_books(
    keyword: str = Query(..., min_length=1),
    page: int = Query(1, ge=1),
    size: int = Query(20, ge=1, le=100),
    db: Session = Depends(get_db)
):
    """æœç´¢ä¹¦ç±"""
    pass
```

#### 3.3.2 æ¦œå•API (rankings.py)
```python
@router.get("/", response_model=List[RankingResponse])
async def get_rankings(
    type: Optional[str] = None,
    active_only: bool = True,
    db: Session = Depends(get_db)
):
    """è·å–æ¦œå•åˆ—è¡¨"""
    pass

@router.get("/{page_id}", response_model=List[RankingResponse])
async def get_rankings_onpage(
    page_id: str,
    type: Optional[str] = None,
    active_only: bool = True,
    db: Session = Depends(get_db)
):
    """è·å–æ¦œå•åˆ—è¡¨"""
    pass

@router.get("/{ranking_id}", response_model=RankingDetailResponse)
async def get_ranking_detail(
    ranking_id: int,
    date: Optional[date] = None,
    limit: int = Query(50, ge=1, le=200),
    db: Session = Depends(get_db)
):
    """è·å–æ¦œå•è¯¦æƒ…"""
    pass

@router.get("/{ranking_id}/trend", response_model=List[RankingHistoryResponse])
async def get_ranking_history(
    ranking_id: int,
    start_date: Optional[date] = None,
    end_date: Optional[date] = None,
    db: Session = Depends(get_db)
):
    """è·å–æ¦œå•å†å²æ•°æ®"""
    pass
```

#### 3.3.3 çˆ¬è™«ç®¡ç†API (crawl.py)
```python
from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException, Query
from typing import List, Optional
from app.scheduler.task_scheduler import get_scheduler
from app.crawler.crawler import JJCrawler

router = APIRouter(prefix="/api/v1/crawl", tags=["crawl"])

@router.post("/all", response_model=CrawlTaskResponse)
async def crawl_all_pages(
    background_tasks: BackgroundTasks,
    force: bool = Query(False, description="æ˜¯å¦å¼ºåˆ¶çˆ¬å–ï¼ˆå¿½ç•¥é—´éš”é™åˆ¶ï¼‰"),
    db: Session = Depends(get_db)
):
    """
    æ‰‹åŠ¨è§¦å‘çˆ¬å–æ‰€æœ‰é…ç½®çš„é¡µé¢
    
    Args:
        force: æ˜¯å¦å¼ºåˆ¶çˆ¬å–ï¼Œå¿½ç•¥æœ€å°é—´éš”é™åˆ¶
        
    Returns:
        CrawlTaskResponse: ä»»åŠ¡ä¿¡æ¯
    """
    try:
        task_id = await scheduler.trigger_crawl_all(force=force)
        return CrawlTaskResponse(
            task_id=task_id,
            status="started",
            message="å·²å¼€å§‹çˆ¬å–æ‰€æœ‰é¡µé¢",
            started_at=datetime.now()
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨çˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}")


@router.post("/page", response_model=CrawlTaskResponse)
async def crawl_multiple_pages(
    request: CrawlPagesRequest,
    background_tasks: BackgroundTasks,
    force: bool = Query(False, description="æ˜¯å¦å¼ºåˆ¶çˆ¬å–ï¼ˆå¿½ç•¥é—´éš”é™åˆ¶ï¼‰"),
    db: Session = Depends(get_db)
):
    """
    æ‰‹åŠ¨è§¦å‘çˆ¬å–å¤šä¸ªæŒ‡å®šé¡µé¢ï¼Œä¹Ÿå¯ä»¥åªçˆ¬å–ä¸€ä¸ªé¡µé¢
    
    Args:
        request: åŒ…å«é¡µé¢IDåˆ—è¡¨çš„è¯·æ±‚ä½“
        force: æ˜¯å¦å¼ºåˆ¶çˆ¬å–ï¼Œå¿½ç•¥æœ€å°é—´éš”é™åˆ¶
        
    Returns:
        CrawlTaskResponse: ä»»åŠ¡ä¿¡æ¯
    """
    pass

@router.get("/tasks", response_model=List[CrawlTaskStatusResponse])
async def get_crawl_tasks(
    status: Optional[str] = Query(None, description="ä»»åŠ¡çŠ¶æ€è¿‡æ»¤"),
    limit: int = Query(50, ge=1, le=200, description="è¿”å›æ•°é‡é™åˆ¶"),
    db: Session = Depends(get_db)
):
    """
    è·å–çˆ¬å–ä»»åŠ¡çŠ¶æ€åˆ—è¡¨
    
    Args:
        status: ä»»åŠ¡çŠ¶æ€è¿‡æ»¤ï¼ˆrunning/completed/failedï¼‰
        limit: è¿”å›æ•°é‡é™åˆ¶
        
    Returns:
        List[CrawlTaskStatusResponse]: ä»»åŠ¡çŠ¶æ€åˆ—è¡¨
    """
    try:
        tasks = await scheduler.get_task_status(status=status, limit=limit)
        return tasks
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")

@router.get("/tasks/{task_id}", response_model=CrawlTaskDetailResponse)
async def get_crawl_task_detail(
    task_id: str,
    db: Session = Depends(get_db)
):
    """
    è·å–æŒ‡å®šçˆ¬å–ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯
    
    Args:
        task_id: ä»»åŠ¡ID
        
    Returns:
        CrawlTaskDetailResponse: ä»»åŠ¡è¯¦ç»†ä¿¡æ¯
    """
    try:
        task_detail = await scheduler.get_task_detail(task_id)
        if not task_detail:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        return task_detail
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {str(e)}")
```

#### 3.3.4 æ•°æ®ä¼ è¾“å¯¹è±¡ (DTOs)
```python
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime

# çˆ¬è™«ç›¸å…³å“åº”æ¨¡å‹
class CrawlTaskResponse(BaseModel):
    """çˆ¬å–ä»»åŠ¡å“åº”"""
    task_id: str = Field(..., description="ä»»åŠ¡ID")
    status: str = Field(..., description="ä»»åŠ¡çŠ¶æ€")
    message: str = Field(..., description="å“åº”æ¶ˆæ¯")
    page_id: Optional[str] = Field(None, description="é¡µé¢ID")
    page_ids: Optional[List[str]] = Field(None, description="é¡µé¢IDåˆ—è¡¨")
    started_at: datetime = Field(..., description="å¼€å§‹æ—¶é—´")

class CrawlTaskStatusResponse(BaseModel):
    """çˆ¬å–ä»»åŠ¡çŠ¶æ€å“åº”"""
    task_id: str = Field(..., description="ä»»åŠ¡ID")
    status: str = Field(..., description="ä»»åŠ¡çŠ¶æ€")
    page_id: Optional[str] = Field(None, description="é¡µé¢ID")
    started_at: datetime = Field(..., description="å¼€å§‹æ—¶é—´")
    completed_at: Optional[datetime] = Field(None, description="å®Œæˆæ—¶é—´")
    error_message: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")
    progress: int = Field(0, ge=0, le=100, description="è¿›åº¦ç™¾åˆ†æ¯”")

class CrawlTaskDetailResponse(BaseModel):
    """çˆ¬å–ä»»åŠ¡è¯¦ç»†ä¿¡æ¯å“åº”"""
    task_id: str = Field(..., description="ä»»åŠ¡ID")
    status: str = Field(..., description="ä»»åŠ¡çŠ¶æ€")
    page_id: Optional[str] = Field(None, description="é¡µé¢ID")
    page_ids: Optional[List[str]] = Field(None, description="é¡µé¢IDåˆ—è¡¨")
    started_at: datetime = Field(..., description="å¼€å§‹æ—¶é—´")
    completed_at: Optional[datetime] = Field(None, description="å®Œæˆæ—¶é—´")
    error_message: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")
    progress: int = Field(0, ge=0, le=100, description="è¿›åº¦ç™¾åˆ†æ¯”")
    crawled_books: int = Field(0, description="å·²çˆ¬å–ä¹¦ç±æ•°é‡")
    total_books: int = Field(0, description="æ€»ä¹¦ç±æ•°é‡")
    execution_time: Optional[float] = Field(None, description="æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰")
    logs: List[str] = Field(default_factory=list, description="ä»»åŠ¡æ—¥å¿—")

class CrawlSystemStatusResponse(BaseModel):
    """çˆ¬è™«ç³»ç»ŸçŠ¶æ€å“åº”"""
    is_running: bool = Field(..., description="ç³»ç»Ÿæ˜¯å¦è¿è¡Œä¸­")
    active_tasks: int = Field(..., description="æ´»è·ƒä»»åŠ¡æ•°é‡")
    total_tasks_today: int = Field(..., description="ä»Šæ—¥æ€»ä»»åŠ¡æ•°")
    success_rate: float = Field(..., description="æˆåŠŸç‡")
    last_crawl_time: Optional[datetime] = Field(None, description="æœ€åçˆ¬å–æ—¶é—´")
    next_scheduled_time: Optional[datetime] = Field(None, description="ä¸‹æ¬¡è°ƒåº¦æ—¶é—´")

class CrawlConfigResponse(BaseModel):
    """çˆ¬è™«é…ç½®å“åº”"""
    user_agent: str = Field(..., description="User Agent")
    timeout: int = Field(..., description="è¶…æ—¶æ—¶é—´")
    retry_times: int = Field(..., description="é‡è¯•æ¬¡æ•°")
    retry_delay: float = Field(..., description="é‡è¯•å»¶è¿Ÿ")
    request_delay: float = Field(..., description="è¯·æ±‚å»¶è¿Ÿ")
    concurrent_requests: int = Field(..., description="å¹¶å‘è¯·æ±‚æ•°")
    pages: List[Dict[str, Any]] = Field(..., description="é¡µé¢é…ç½®åˆ—è¡¨")

# çˆ¬è™«ç›¸å…³è¯·æ±‚æ¨¡å‹
class CrawlMultiplePagesRequest(BaseModel):
    """çˆ¬å–å¤šä¸ªé¡µé¢è¯·æ±‚"""
    page_ids: List[str] = Field(..., min_items=1, description="é¡µé¢IDåˆ—è¡¨")

class UpdateCrawlConfigRequest(BaseModel):
    """æ›´æ–°çˆ¬è™«é…ç½®è¯·æ±‚"""
    user_agent: Optional[str] = Field(None, description="User Agent")
    timeout: Optional[int] = Field(None, ge=1, le=300, description="è¶…æ—¶æ—¶é—´")
    retry_times: Optional[int] = Field(None, ge=0, le=10, description="é‡è¯•æ¬¡æ•°")
    retry_delay: Optional[float] = Field(None, ge=0, le=60, description="é‡è¯•å»¶è¿Ÿ")
    request_delay: Optional[float] = Field(None, ge=0, le=10, description="è¯·æ±‚å»¶è¿Ÿ")
         concurrent_requests: Optional[int] = Field(None, ge=1, le=20, description="å¹¶å‘è¯·æ±‚æ•°")
```

### 3.4 çˆ¬è™«æ¨¡å—è®¾è®¡

#### 3.4.1 çˆ¬è™«æ¶æ„
```python
# ç»Ÿä¸€çˆ¬è™«æ¥å£
class JJCrawler:
    """æ™‹æ±Ÿæ–‡å­¦åŸçˆ¬è™«ä¸»ç±»"""
    
    def __init__(self, config: CrawlerConfig):
        self.config = config
        self.session = self._create_session()
        self.parsers = self._load_parsers()
    
    async def crawl_page(self, page_id: str, force: bool = False) -> CrawlResult:
        """çˆ¬å–å•ä¸ªé¡µé¢"""
        
    async def crawl_pages(self, page_ids: List[str], force: bool = False) -> List[CrawlResult]:
        """çˆ¬å–å¤šä¸ªé¡µé¢"""
        
    async def crawl_all(self, force: bool = False) -> List[CrawlResult]:
        """çˆ¬å–æ‰€æœ‰é…ç½®çš„é¡µé¢"""

# é¡µé¢è§£æå™¨æ¥å£
class BaseParser:
    """åŸºç¡€è§£æå™¨"""
    
    async def parse(self, html: str, page_config: dict) -> List[dict]:
        """è§£æé¡µé¢å†…å®¹"""
        raise NotImplementedError

class JiaziParser(BaseParser):
    """å¤¹å­æ¦œä¸“ç”¨è§£æå™¨"""
    
    async def parse(self, html: str, page_config: dict) -> List[dict]:
        """è§£æå¤¹å­æ¦œé¡µé¢"""
        # å¤¹å­æ¦œç‰¹æœ‰çš„è§£æé€»è¾‘
        
class CategoryParser(BaseParser):
    """åˆ†ç±»æ¦œå•è§£æå™¨"""
    
    async def parse(self, html: str, page_config: dict) -> List[dict]:
        """è§£æåˆ†ç±»æ¦œå•é¡µé¢"""
        # åˆ†ç±»æ¦œå•é€šç”¨è§£æé€»è¾‘
```

#### 3.4.2 çˆ¬è™«ç­–ç•¥è®¾è®¡
```python
# çˆ¬å–ç­–ç•¥é…ç½®
CRAWL_STRATEGIES = {
    "jiazi": {
        "interval": 3600,  # 1å°æ—¶é—´éš”
        "priority": 1,     # é«˜ä¼˜å…ˆçº§
        "parser": "JiaziParser",
        "retry_on_fail": True
    },
    "category_pages": {
        "interval": 7200,  # 2å°æ—¶é—´éš”
        "priority": 2,     # ä¸­ä¼˜å…ˆçº§
        "parser": "CategoryParser",
        "retry_on_fail": True
    }
}

# ä»»åŠ¡è°ƒåº¦å™¨
class TaskScheduler:
    """ä»»åŠ¡è°ƒåº¦å™¨"""
    
    async def trigger_crawl_all(self, force: bool = False) -> str:
        """è§¦å‘çˆ¬å–æ‰€æœ‰é¡µé¢"""
        task_id = self._generate_task_id()
        
        # æ£€æŸ¥é—´éš”é™åˆ¶
        if not force and not self._check_interval_allowed():
            raise ValueError("è·ç¦»ä¸Šæ¬¡çˆ¬å–æ—¶é—´è¿‡çŸ­")
        
        # åˆ›å»ºåå°ä»»åŠ¡
        background_task = BackgroundTask(
            func=self._execute_crawl_all,
            task_id=task_id
        )
        
        self._submit_task(background_task)
        return task_id
    
    async def trigger_crawl_page(self, page_id: str, force: bool = False) -> str:
        """è§¦å‘çˆ¬å–å•ä¸ªé¡µé¢"""
        if page_id not in self.page_configs:
            raise ValueError(f"é¡µé¢ {page_id} ä¸å­˜åœ¨")
        
        task_id = self._generate_task_id()
        
        # æ£€æŸ¥é¡µé¢ç‰¹å®šçš„é—´éš”é™åˆ¶
        if not force and not self._check_page_interval(page_id):
            raise ValueError(f"é¡µé¢ {page_id} è·ç¦»ä¸Šæ¬¡çˆ¬å–æ—¶é—´è¿‡çŸ­")
        
        background_task = BackgroundTask(
            func=self._execute_crawl_page,
            task_id=task_id,
            page_id=page_id
        )
        
        self._submit_task(background_task)
        return task_id
    
    async def trigger_crawl_pages(self, page_ids: List[str], force: bool = False) -> str:
        """è§¦å‘çˆ¬å–å¤šä¸ªé¡µé¢"""
        # éªŒè¯æ‰€æœ‰é¡µé¢ID
        invalid_pages = [pid for pid in page_ids if pid not in self.page_configs]
        if invalid_pages:
            raise ValueError(f"é¡µé¢ä¸å­˜åœ¨: {invalid_pages}")
        
        task_id = self._generate_task_id()
        
        # æ£€æŸ¥æ‰¹é‡é—´éš”é™åˆ¶
        if not force:
            blocked_pages = [pid for pid in page_ids if not self._check_page_interval(pid)]
            if blocked_pages:
                raise ValueError(f"ä»¥ä¸‹é¡µé¢è·ç¦»ä¸Šæ¬¡çˆ¬å–æ—¶é—´è¿‡çŸ­: {blocked_pages}")
        
        background_task = BackgroundTask(
            func=self._execute_crawl_pages,
            task_id=task_id,
            page_ids=page_ids
        )
        
        self._submit_task(background_task)
        return task_id
```

#### 3.4.3 çˆ¬è™«é…ç½®ç®¡ç†
```python
# é¡µé¢é…ç½®ç»“æ„
PAGE_CONFIGS = {
    "jiazi": {
        "name": "å¤¹å­æ¦œ",
        "url": "https://www.jjwxc.net/topten.php?orderstr=4",
        "parser": "JiaziParser",
        "interval": 3600,
        "enabled": True,
        "headers": {
            "User-Agent": "Mozilla/5.0...",
            "Referer": "https://www.jjwxc.net/"
        }
    },
    "nvpin_wcxl": {
        "name": "å¥³é¢‘-å®Œç»“ç°è¨€",
        "url": "https://www.jjwxc.net/bookbase.php?fw0=0&fbsj0=0&ycx0=0&xx0=0&mainview0=0&sd0=0&lx0=0&fg0=0&sortType=0&isfinish=1&collectiontypes=ors&searchkeywords=&page=1",
        "parser": "CategoryParser",
        "interval": 7200,
        "enabled": True
    }
    # ... å…¶ä»–é¡µé¢é…ç½®
}

# åŠ¨æ€é…ç½®ç®¡ç†
class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    async def get_page_config(self, page_id: str) -> dict:
        """è·å–é¡µé¢é…ç½®"""
        
    async def update_page_config(self, page_id: str, config: dict) -> bool:
        """æ›´æ–°é¡µé¢é…ç½®"""
        
    async def get_all_pages(self) -> List[dict]:
        """è·å–æ‰€æœ‰é¡µé¢é…ç½®"""
        
    async def get_enabled_pages(self) -> List[dict]:
        """è·å–å¯ç”¨çš„é¡µé¢é…ç½®"""
```

#### 3.4.4 ä»»åŠ¡çŠ¶æ€ç®¡ç†
```python
# ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª
class TaskTracker:
    """ä»»åŠ¡çŠ¶æ€è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        self.tasks: Dict[str, TaskInfo] = {}
        self.task_history: List[TaskInfo] = []
    
    async def create_task(self, task_id: str, task_type: str, **kwargs) -> None:
        """åˆ›å»ºæ–°ä»»åŠ¡"""
        
    async def update_task_status(self, task_id: str, status: str, **kwargs) -> None:
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        
    async def get_task_status(self, task_id: str) -> Optional[TaskInfo]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        
    async def get_active_tasks(self) -> List[TaskInfo]:
        """è·å–æ´»è·ƒä»»åŠ¡åˆ—è¡¨"""
        
    async def get_task_history(self, limit: int = 50) -> List[TaskInfo]:
        """è·å–ä»»åŠ¡å†å²"""

# ä»»åŠ¡ä¿¡æ¯æ¨¡å‹
@dataclass
class TaskInfo:
    task_id: str
    task_type: str  # "crawl_all", "crawl_page", "crawl_pages"
    status: str     # "pending", "running", "completed", "failed", "cancelled"
    page_id: Optional[str] = None
    page_ids: Optional[List[str]] = None
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = None
    progress: int = 0
    crawled_books: int = 0
    total_books: int = 0
    execution_time: Optional[float] = None
    logs: List[str] = field(default_factory=list)
```

#### 3.4.5 çˆ¬è™«é”™è¯¯å¤„ç†ä¸é‡è¯•æœºåˆ¶
```python
# çˆ¬è™«ä¸“ç”¨å¼‚å¸¸
class CrawlerException(Exception):
    """çˆ¬è™«åŸºç¡€å¼‚å¸¸"""
    pass

class NetworkException(CrawlerException):
    """ç½‘ç»œå¼‚å¸¸"""
    pass

class ParsingException(CrawlerException):
    """è§£æå¼‚å¸¸"""
    pass

class RateLimitException(CrawlerException):
    """é¢‘ç‡é™åˆ¶å¼‚å¸¸"""
    pass

# é‡è¯•ç­–ç•¥
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type((NetworkException, RateLimitException))
)
async def crawl_with_retry(url: str, headers: dict) -> str:
    """å¸¦é‡è¯•çš„çˆ¬å–å‡½æ•°"""
    async with httpx.AsyncClient() as client:
        response = await client.get(url, headers=headers, timeout=30)
        
        if response.status_code == 429:  # é¢‘ç‡é™åˆ¶
            raise RateLimitException("è¯·æ±‚é¢‘ç‡è¿‡é«˜")
        elif response.status_code >= 500:  # æœåŠ¡å™¨é”™è¯¯
            raise NetworkException(f"æœåŠ¡å™¨é”™è¯¯: {response.status_code}")
        elif response.status_code != 200:
            raise NetworkException(f"HTTPé”™è¯¯: {response.status_code}")
        
        return response.text
```

## 4. é”™è¯¯å¤„ç†ä¸æ—¥å¿—

### 4.1 å¼‚å¸¸å¤„ç†ç­–ç•¥
```python
# è‡ªå®šä¹‰å¼‚å¸¸ç±»
class JJCrawlerException(Exception):
    """åŸºç¡€å¼‚å¸¸ç±»"""
    pass

class BookNotFoundError(JJCrawlerException):
    """ä¹¦ç±ä¸å­˜åœ¨å¼‚å¸¸"""
    pass

class CrawlerError(JJCrawlerException):
    """çˆ¬è™«å¼‚å¸¸"""
    pass

class DatabaseError(JJCrawlerException):
    """æ•°æ®åº“å¼‚å¸¸"""
    pass
```

### 4.2 æ—¥å¿—é…ç½®
```python
# æ—¥å¿—é…ç½®
LOGGING_CONFIG = {
    "version": 1,
    "disable_existing_loggers": False,
    "formatters": {
        "default": {
            "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        },
        "detailed": {
            "format": "%(asctime)s - %(name)s - %(levelname)s - %(module)s - %(funcName)s - %(lineno)d - %(message)s"
        }
    },
    "handlers": {
        "console": {
            "class": "logging.StreamHandler",
            "level": "INFO",
            "formatter": "default"
        },
        "file": {
            "class": "logging.handlers.RotatingFileHandler",
            "level": "DEBUG",
            "formatter": "detailed",
            "filename": "logs/jjcrawler.log",
            "maxBytes": 10485760,
            "backupCount": 5
        }
    },
    "loggers": {
        "jjcrawler": {
            "level": "DEBUG",
            "handlers": ["console", "file"]
        }
    }
}
```

## 5. æ€§èƒ½ä¼˜åŒ–

### 5.1 æ•°æ®åº“ä¼˜åŒ–
- **ç´¢å¼•ç­–ç•¥**: æ ¹æ®æŸ¥è¯¢æ¨¡å¼åˆ›å»ºåˆé€‚çš„ç´¢å¼•
- **æŸ¥è¯¢ä¼˜åŒ–**: ä½¿ç”¨SQLAlchemyçš„æŸ¥è¯¢ä¼˜åŒ–ç‰¹æ€§
- **è¿æ¥æ± **: é…ç½®åˆé€‚çš„æ•°æ®åº“è¿æ¥æ± 
- **æ‰¹é‡æ“ä½œ**: ä½¿ç”¨æ‰¹é‡æ’å…¥å’Œæ›´æ–°å‡å°‘æ•°æ®åº“äº¤äº’

### 5.2 ç¼“å­˜ç­–ç•¥
```python
# Redisç¼“å­˜é…ç½®
CACHE_CONFIG = {
    "backend": "redis",
    "host": "localhost",
    "port": 6379,
    "db": 0,
    "expire": 3600  # 1å°æ—¶è¿‡æœŸ
}

# ç¼“å­˜è£…é¥°å™¨
@cache.cached(timeout=3600)
async def get_book_detail(novel_id: int):
    """è·å–ä¹¦ç±è¯¦æƒ…ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    pass
```

## 6. éƒ¨ç½²æ¶æ„

### 6.1 Docker Composeé…ç½®
```yaml
version: '3.8'
services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=sqlite:///./data/jjcrawler.db
      - API_DEBUG=false
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
  
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    depends_on:
      - app
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
```

## 7. æµ‹è¯•ç­–ç•¥

### 7.1 æµ‹è¯•é‡‘å­—å¡”
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ç«¯åˆ°ç«¯æµ‹è¯• (E2E)  â”‚  â† å°‘é‡ï¼Œè¦†ç›–å…³é”®ä¸šåŠ¡æµç¨‹
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    é›†æˆæµ‹è¯•          â”‚  â† é€‚é‡ï¼Œæµ‹è¯•æ¨¡å—é—´äº¤äº’
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    å•å…ƒæµ‹è¯•          â”‚  â† å¤§é‡ï¼Œæµ‹è¯•ä¸ªåˆ«å‡½æ•°/æ–¹æ³•
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 æµ‹è¯•è¦†ç›–ç‡ç›®æ ‡
- **å•å…ƒæµ‹è¯•**: 90%ä»¥ä¸Šä»£ç è¦†ç›–ç‡
- **é›†æˆæµ‹è¯•**: 80%ä»¥ä¸Šæ¥å£è¦†ç›–ç‡
- **ç«¯åˆ°ç«¯æµ‹è¯•**: 100%å…³é”®ä¸šåŠ¡æµç¨‹è¦†ç›–

### 7.3 æµ‹è¯•æ•°æ®ç®¡ç†
```python
# æµ‹è¯•æ•°æ®å·¥å‚
class BookFactory:
    @staticmethod
    def create_book(**kwargs):
        defaults = {
            "novel_id": 12345,
            "title": "æµ‹è¯•å°è¯´",
            "author": "æµ‹è¯•ä½œè€…",
            "status": "è¿è½½ä¸­",
            "tag": "ç°ä»£è¨€æƒ…",
            "length": 100000,
            "intro": "æµ‹è¯•å°è¯´ç®€ä»‹",
            "last_update": datetime.now()
        }
        defaults.update(kwargs)
        return Book(**defaults)
```

## 8. æ‰©å±•è§„åˆ’

### 8.1 åŠŸèƒ½æ‰©å±•
- **å¤šæ•°æ®æº**: æ”¯æŒæ›´å¤šæ–‡å­¦ç½‘ç«™
- **å®æ—¶é€šçŸ¥**: ä¹¦ç±æ›´æ–°å®æ—¶æ¨é€
- **æ•°æ®åˆ†æ**: è¶‹åŠ¿åˆ†æå’Œé¢„æµ‹
- **ç”¨æˆ·ç³»ç»Ÿ**: ç”¨æˆ·æ³¨å†Œå’Œä¸ªæ€§åŒ–æ¨è

### 8.2 æŠ€æœ¯å‡çº§
- **å¾®æœåŠ¡æ¶æ„**: æ‹†åˆ†ä¸ºå¤šä¸ªå¾®æœåŠ¡
- **å®¹å™¨åŒ–**: å®Œå…¨å®¹å™¨åŒ–éƒ¨ç½²
- **æ¶ˆæ¯é˜Ÿåˆ—**: å¼•å…¥Redis/RabbitMQ
- **åˆ†å¸ƒå¼**: æ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²

### 8.3 æ€§èƒ½æå‡
- **ç¼“å­˜å±‚**: å¼•å…¥Redisç¼“å­˜
- **CDN**: é™æ€èµ„æºCDNåŠ é€Ÿ
- **è´Ÿè½½å‡è¡¡**: å¤šå®ä¾‹è´Ÿè½½å‡è¡¡
- **æ•°æ®åº“ä¼˜åŒ–**: è¯»å†™åˆ†ç¦»ã€åˆ†åº“åˆ†è¡¨

## 9. æ€»ç»“

æœ¬è®¾è®¡æ–¹æ¡ˆåŸºäºç°ä»£åŒ–çš„PythonæŠ€æœ¯æ ˆï¼Œé‡‡ç”¨MVPæ¶æ„åŸåˆ™ï¼Œæ—¢ä¿è¯äº†åŠŸèƒ½çš„å®Œæ•´æ€§ï¼Œåˆç¡®ä¿äº†ç³»ç»Ÿçš„å¯ç»´æŠ¤æ€§å’Œå¯æ‰©å±•æ€§ã€‚é€šè¿‡åˆ†å±‚è®¾è®¡ã€æ¨¡å—åŒ–å¼€å‘ã€å®Œå–„çš„æµ‹è¯•ç­–ç•¥å’Œè¯¦ç»†çš„æ–‡æ¡£ï¼Œä¸ºé¡¹ç›®çš„æˆåŠŸå®æ–½æä¾›äº†åšå®çš„åŸºç¡€ã€‚

é¡¹ç›®çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š
- **ç±»å‹å®‰å…¨**: å…¨é¢ä½¿ç”¨ç±»å‹æ³¨è§£å’ŒéªŒè¯
- **é«˜æ•ˆæ€§èƒ½**: å¼‚æ­¥å¤„ç†å’Œæ•°æ®åº“ä¼˜åŒ–
- **æ˜“äºç»´æŠ¤**: æ¸…æ™°çš„æ¨¡å—åˆ’åˆ†å’Œä»£ç ç»“æ„
- **å¯æ‰©å±•æ€§**: æ”¯æŒåŠŸèƒ½å’ŒæŠ€æœ¯çš„æ¸è¿›å¼å‡çº§
- **å®Œæ•´æµ‹è¯•**: å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€ç«¯åˆ°ç«¯æµ‹è¯•å…¨è¦†ç›–

è¿™ä¸ªè®¾è®¡æ–¹æ¡ˆå°†ä¸ºJJCrawleré¡¹ç›®æä¾›ä¸€ä¸ªç¨³å®šã€é«˜æ•ˆã€å¯ç»´æŠ¤çš„æŠ€æœ¯åŸºç¡€ï¼Œæ”¯æ’‘é¡¹ç›®çš„é•¿æœŸå‘å±•ã€‚
