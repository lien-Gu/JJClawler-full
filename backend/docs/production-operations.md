# ç”Ÿäº§ç¯å¢ƒè¿ç»´æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾› JJCrawler3 ç”Ÿäº§ç¯å¢ƒçš„å®Œæ•´è¿ç»´æŒ‡å—ï¼ŒåŒ…æ‹¬åº”ç”¨ç›‘æ§ã€æ—¥å¿—ç®¡ç†ã€æ•°æ®å¤‡ä»½ã€ç³»ç»Ÿç»´æŠ¤å’Œæ•…éšœå¤„ç†ç­‰å…³é”®è¿ç»´æ“ä½œã€‚

## ç›®å½•

1. [åº”ç”¨ç›‘æ§](#åº”ç”¨ç›‘æ§)
2. [æ—¥å¿—ç®¡ç†](#æ—¥å¿—ç®¡ç†)
3. [æ•°æ®å¤‡ä»½ä¸æ¢å¤](#æ•°æ®å¤‡ä»½ä¸æ¢å¤)
4. [ç³»ç»Ÿç»´æŠ¤](#ç³»ç»Ÿç»´æŠ¤)
5. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
6. [å®‰å…¨ç®¡ç†](#å®‰å…¨ç®¡ç†)
7. [æ•…éšœå¤„ç†](#æ•…éšœå¤„ç†)
8. [å‡çº§éƒ¨ç½²](#å‡çº§éƒ¨ç½²)

## åº”ç”¨ç›‘æ§

### 1. å¥åº·æ£€æŸ¥ç›‘æ§

#### åŸºç¡€å¥åº·æ£€æŸ¥

```bash
#!/bin/bash
# health_check.sh - åŸºç¡€å¥åº·æ£€æŸ¥è„šæœ¬

# é…ç½®å‚æ•°
SERVICE_URL="http://localhost:8000"
ALERT_EMAIL="admin@example.com"
LOG_FILE="/var/log/jjcrawler/health.log"

# å¥åº·æ£€æŸ¥å‡½æ•°
check_service_health() {
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    # API å¥åº·æ£€æŸ¥
    if curl -s -f "${SERVICE_URL}/health" > /dev/null; then
        echo "[$timestamp] âœ… API æœåŠ¡æ­£å¸¸" | tee -a $LOG_FILE
        return 0
    else
        echo "[$timestamp] âŒ API æœåŠ¡å¼‚å¸¸" | tee -a $LOG_FILE
        return 1
    fi
}

# è¯¦ç»†çŠ¶æ€æ£€æŸ¥
check_detailed_status() {
    local response=$(curl -s "${SERVICE_URL}/stats")
    local status=$(echo $response | jq -r '.status')
    
    if [ "$status" = "ok" ]; then
        # æ£€æŸ¥å…³é”®æŒ‡æ ‡
        local crawler_stats=$(echo $response | jq -r '.crawler_stats')
        local scheduler_stats=$(echo $response | jq -r '.scheduler_stats')
        
        echo "çˆ¬è™«ç»Ÿè®¡: $crawler_stats"
        echo "è°ƒåº¦å™¨ç»Ÿè®¡: $scheduler_stats"
    else
        echo "æœåŠ¡çŠ¶æ€å¼‚å¸¸: $status"
        return 1
    fi
}

# æ‰§è¡Œæ£€æŸ¥
if check_service_health; then
    check_detailed_status
else
    # å‘é€å‘Šè­¦
    echo "JJCrawler3 æœåŠ¡å¼‚å¸¸ï¼Œè¯·ç«‹å³æ£€æŸ¥" | mail -s "æœåŠ¡å‘Šè­¦" $ALERT_EMAIL
    exit 1
fi
```

#### é«˜çº§ç›‘æ§æŒ‡æ ‡

```bash
#!/bin/bash
# advanced_monitor.sh - é«˜çº§ç›‘æ§è„šæœ¬

# ç›‘æ§é…ç½®
THRESHOLDS=(
    "memory_usage:80"      # å†…å­˜ä½¿ç”¨ç‡é˜ˆå€¼ 80%
    "cpu_usage:70"         # CPUä½¿ç”¨ç‡é˜ˆå€¼ 70%
    "disk_usage:85"        # ç£ç›˜ä½¿ç”¨ç‡é˜ˆå€¼ 85%
    "response_time:5000"   # å“åº”æ—¶é—´é˜ˆå€¼ 5ç§’
    "error_rate:5"         # é”™è¯¯ç‡é˜ˆå€¼ 5%
)

# è·å–å®¹å™¨èµ„æºä½¿ç”¨æƒ…å†µ
get_container_stats() {
    docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}" jjcrawler
}

# æ£€æŸ¥å“åº”æ—¶é—´
check_response_time() {
    local start_time=$(date +%s%3N)
    curl -s "${SERVICE_URL}/health" > /dev/null
    local end_time=$(date +%s%3N)
    local response_time=$((end_time - start_time))
    
    echo "å“åº”æ—¶é—´: ${response_time}ms"
    
    if [ $response_time -gt 5000 ]; then
        echo "âš ï¸  å“åº”æ—¶é—´è¿‡é•¿: ${response_time}ms"
        return 1
    fi
    return 0
}

# æ£€æŸ¥é”™è¯¯ç‡
check_error_rate() {
    local stats=$(curl -s "${SERVICE_URL}/api/v1/crawl/scheduler/stats")
    local total_executed=$(echo $stats | jq -r '.total_executed // 0')
    local total_failed=$(echo $stats | jq -r '.total_failed // 0')
    
    if [ $total_executed -gt 0 ]; then
        local error_rate=$(echo "scale=2; $total_failed * 100 / $total_executed" | bc)
        echo "é”™è¯¯ç‡: ${error_rate}%"
        
        if (( $(echo "$error_rate > 5" | bc -l) )); then
            echo "âš ï¸  é”™è¯¯ç‡è¿‡é«˜: ${error_rate}%"
            return 1
        fi
    fi
    return 0
}

# æ£€æŸ¥ç£ç›˜ç©ºé—´
check_disk_space() {
    local usage=$(df -h /app/data | awk 'NR==2 {print $5}' | sed 's/%//')
    echo "æ•°æ®ç›®å½•ç£ç›˜ä½¿ç”¨ç‡: ${usage}%"
    
    if [ $usage -gt 85 ]; then
        echo "âš ï¸  ç£ç›˜ç©ºé—´ä¸è¶³: ${usage}%"
        return 1
    fi
    return 0
}

# æ‰§è¡Œå…¨é¢ç›‘æ§
main() {
    echo "=== JJCrawler3 é«˜çº§ç›‘æ§ $(date) ==="
    
    local issues=0
    
    echo "ğŸ“Š å®¹å™¨èµ„æºä½¿ç”¨:"
    get_container_stats
    
    echo -e "\nğŸ• å“åº”æ—¶é—´æ£€æŸ¥:"
    check_response_time || ((issues++))
    
    echo -e "\nğŸ“ˆ é”™è¯¯ç‡æ£€æŸ¥:"
    check_error_rate || ((issues++))
    
    echo -e "\nğŸ’¾ ç£ç›˜ç©ºé—´æ£€æŸ¥:"
    check_disk_space || ((issues++))
    
    echo -e "\nğŸ“‹ æ€»ç»“:"
    if [ $issues -eq 0 ]; then
        echo "âœ… æ‰€æœ‰ç›‘æ§æŒ‡æ ‡æ­£å¸¸"
    else
        echo "âŒ å‘ç° $issues ä¸ªç›‘æ§å‘Šè­¦"
        # å‘é€å‘Šè­¦é€šçŸ¥
        send_alert "å‘ç° $issues ä¸ªç›‘æ§å‘Šè­¦ï¼Œéœ€è¦æ£€æŸ¥"
    fi
}

# å‘Šè­¦é€šçŸ¥å‡½æ•°
send_alert() {
    local message="$1"
    # é’‰é’‰æœºå™¨äººé€šçŸ¥
    curl -H "Content-Type: application/json" \
         -d "{\"msgtype\":\"text\",\"text\":{\"content\":\"JJCrawler3å‘Šè­¦: $message\"}}" \
         "$DINGTALK_WEBHOOK"
}

main
```

### 2. è‡ªåŠ¨åŒ–ç›‘æ§éƒ¨ç½²

#### Prometheus + Grafana ç›‘æ§

åˆ›å»º `docker-compose.monitoring.yml`ï¼š

```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: jjcrawler-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'

  grafana:
    image: grafana/grafana:latest
    container_name: jjcrawler-grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
      - ./monitoring/grafana-dashboards.yml:/etc/grafana/provisioning/dashboards/dashboards.yml:ro
      - ./monitoring/dashboards:/var/lib/grafana/dashboards:ro
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false

  node-exporter:
    image: prom/node-exporter:latest
    container_name: jjcrawler-node-exporter
    restart: unless-stopped
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'

volumes:
  prometheus_data:
  grafana_data:
```

#### Prometheus é…ç½®

`monitoring/prometheus.yml`ï¼š

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "jjcrawler_rules.yml"

scrape_configs:
  - job_name: 'jjcrawler'
    static_configs:
      - targets: ['jjcrawler:8000']
    metrics_path: '/metrics'
    scrape_interval: 30s

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

### 3. å®æ—¶ç›‘æ§é¢æ¿

#### è‡ªå®šä¹‰ç›‘æ§ç«¯ç‚¹

åœ¨ FastAPI åº”ç”¨ä¸­æ·»åŠ ç›‘æ§ç«¯ç‚¹ï¼š

```python
# app/api/monitoring.py
from fastapi import APIRouter
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import psutil
import time

router = APIRouter()

# Prometheus æŒ‡æ ‡
REQUEST_COUNT = Counter('jjcrawler_requests_total', 'Total requests', ['method', 'endpoint'])
REQUEST_DURATION = Histogram('jjcrawler_request_duration_seconds', 'Request duration')
TASK_COUNT = Counter('jjcrawler_tasks_total', 'Total tasks', ['type', 'status'])
MEMORY_USAGE = Gauge('jjcrawler_memory_usage_bytes', 'Memory usage in bytes')
CPU_USAGE = Gauge('jjcrawler_cpu_usage_percent', 'CPU usage percentage')

@router.get("/metrics")
async def get_metrics():
    """Prometheus æŒ‡æ ‡ç«¯ç‚¹"""
    # æ›´æ–°ç³»ç»ŸæŒ‡æ ‡
    MEMORY_USAGE.set(psutil.virtual_memory().used)
    CPU_USAGE.set(psutil.cpu_percent())
    
    return Response(generate_latest(), media_type="text/plain")

@router.get("/monitoring/dashboard")
async def get_monitoring_dashboard():
    """ç›‘æ§é¢æ¿æ•°æ®"""
    from app.modules.service.scheduler_service import get_scheduler_service
    from app.modules.service.task_service import get_task_manager
    
    scheduler_service = get_scheduler_service()
    task_manager = get_task_manager()
    
    return {
        "timestamp": time.time(),
        "system": {
            "memory_usage": psutil.virtual_memory()._asdict(),
            "cpu_usage": psutil.cpu_percent(interval=1),
            "disk_usage": psutil.disk_usage('/app/data')._asdict()
        },
        "scheduler": scheduler_service.get_job_statistics(),
        "tasks": task_manager.get_task_summary(),
        "active_jobs": len(scheduler_service.get_scheduled_jobs()) if scheduler_service.scheduler else 0
    }
```

## æ—¥å¿—ç®¡ç†

### 1. æ—¥å¿—é…ç½®å’Œæ”¶é›†

#### ç»“æ„åŒ–æ—¥å¿—é…ç½®

```python
# app/utils/log_utils.py å¢å¼ºç‰ˆ
import logging
import logging.handlers
import json
from datetime import datetime
from typing import Dict, Any

class StructuredFormatter(logging.Formatter):
    """ç»“æ„åŒ–æ—¥å¿—æ ¼å¼å™¨"""
    
    def format(self, record):
        log_data = {
            "timestamp": datetime.fromtimestamp(record.created).isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }
        
        # æ·»åŠ å¼‚å¸¸ä¿¡æ¯
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)
        
        # æ·»åŠ è‡ªå®šä¹‰å­—æ®µ
        if hasattr(record, 'task_id'):
            log_data["task_id"] = record.task_id
        if hasattr(record, 'user_id'):
            log_data["user_id"] = record.user_id
            
        return json.dumps(log_data, ensure_ascii=False)

def setup_production_logging():
    """ç”Ÿäº§ç¯å¢ƒæ—¥å¿—é…ç½®"""
    
    # æ ¹æ—¥å¿—å™¨é…ç½®
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    
    # æ§åˆ¶å°å¤„ç†å™¨ (ç»“æ„åŒ–è¾“å‡º)
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(StructuredFormatter())
    root_logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨ (æŒ‰å¤§å°æ»šåŠ¨)
    file_handler = logging.handlers.RotatingFileHandler(
        filename="/app/logs/jjcrawler.log",
        maxBytes=100 * 1024 * 1024,  # 100MB
        backupCount=10,
        encoding='utf-8'
    )
    file_handler.setFormatter(StructuredFormatter())
    root_logger.addHandler(file_handler)
    
    # é”™è¯¯æ—¥å¿—å¤„ç†å™¨
    error_handler = logging.handlers.RotatingFileHandler(
        filename="/app/logs/jjcrawler-error.log",
        maxBytes=50 * 1024 * 1024,  # 50MB
        backupCount=5,
        encoding='utf-8'
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(StructuredFormatter())
    root_logger.addHandler(error_handler)
    
    # ä»»åŠ¡ä¸“ç”¨æ—¥å¿—
    task_logger = logging.getLogger('jjcrawler.tasks')
    task_handler = logging.handlers.TimedRotatingFileHandler(
        filename="/app/logs/tasks.log",
        when='midnight',
        interval=1,
        backupCount=30,
        encoding='utf-8'
    )
    task_handler.setFormatter(StructuredFormatter())
    task_logger.addHandler(task_handler)
```

#### Docker æ—¥å¿—é…ç½®

```yaml
# docker-compose.yml æ—¥å¿—é…ç½®
services:
  jjcrawler:
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"
        labels: "service=jjcrawler,environment=production"
    
    # æˆ–ä½¿ç”¨ syslog é©±åŠ¨
    # logging:
    #   driver: "syslog"
    #   options:
    #     syslog-address: "tcp://logstash:5000"
    #     tag: "jjcrawler"
```

### 2. æ—¥å¿—èšåˆå’Œåˆ†æ

#### ELK Stack é›†æˆ

```yaml
# docker-compose.elk.yml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.15.0
    container_name: jjcrawler-elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"

  logstash:
    image: docker.elastic.co/logstash/logstash:7.15.0
    container_name: jjcrawler-logstash
    volumes:
      - ./elk/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro
    ports:
      - "5000:5000"
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:7.15.0
    container_name: jjcrawler-kibana
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch

volumes:
  elasticsearch_data:
```

#### Logstash é…ç½®

```ruby
# elk/logstash.conf
input {
  tcp {
    port => 5000
    codec => json_lines
  }
  
  beats {
    port => 5044
  }
}

filter {
  if [message] =~ /^{.*}$/ {
    json {
      source => "message"
    }
  }
  
  # è§£æ JJCrawler ç‰¹å®šå­—æ®µ
  if [logger] =~ /jjcrawler/ {
    mutate {
      add_field => { "service" => "jjcrawler" }
    }
  }
  
  # æ·»åŠ æ—¶é—´æˆ³
  date {
    match => [ "timestamp", "ISO8601" ]
    target => "@timestamp"
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "jjcrawler-logs-%{+YYYY.MM.dd}"
  }
  
  # è¾“å‡ºåˆ°æ§åˆ¶å°ç”¨äºè°ƒè¯•
  stdout { 
    codec => rubydebug 
  }
}
```

### 3. æ—¥å¿—è¿ç»´è„šæœ¬

#### æ—¥å¿—åˆ†æè„šæœ¬

```bash
#!/bin/bash
# log_analysis.sh - æ—¥å¿—åˆ†æå·¥å…·

LOG_DIR="/app/logs"
REPORT_FILE="/tmp/log_report_$(date +%Y%m%d).txt"

analyze_error_patterns() {
    echo "=== é”™è¯¯æ¨¡å¼åˆ†æ ===" > $REPORT_FILE
    echo "æ—¶é—´èŒƒå›´: $(date)" >> $REPORT_FILE
    echo "" >> $REPORT_FILE
    
    # åˆ†ææœ€è¿‘24å°æ—¶çš„é”™è¯¯
    find $LOG_DIR -name "*.log" -mtime -1 -exec grep -h "ERROR" {} \; | \
    awk -F'"message":"' '{print $2}' | \
    awk -F'"' '{print $1}' | \
    sort | uniq -c | sort -nr | head -10 >> $REPORT_FILE
    
    echo "" >> $REPORT_FILE
}

analyze_performance() {
    echo "=== æ€§èƒ½åˆ†æ ===" >> $REPORT_FILE
    
    # åˆ†æä»»åŠ¡æ‰§è¡Œæ—¶é—´
    grep "ä»»åŠ¡æ‰§è¡Œå®Œæˆ" $LOG_DIR/tasks.log | \
    grep -o "è€—æ—¶: [0-9.]*ç§’" | \
    awk '{print $2}' | sort -n | \
    awk '
    {
        values[NR] = $1
        sum += $1
    }
    END {
        avg = sum / NR
        median = values[int(NR/2)]
        print "å¹³å‡æ‰§è¡Œæ—¶é—´: " avg "ç§’"
        print "ä¸­ä½æ•°æ‰§è¡Œæ—¶é—´: " median "ç§’"
        print "æœ€é•¿æ‰§è¡Œæ—¶é—´: " values[NR] "ç§’"
    }' >> $REPORT_FILE
    
    echo "" >> $REPORT_FILE
}

check_disk_usage() {
    echo "=== æ—¥å¿—ç£ç›˜ä½¿ç”¨æƒ…å†µ ===" >> $REPORT_FILE
    du -sh $LOG_DIR/* | sort -hr >> $REPORT_FILE
    echo "" >> $REPORT_FILE
}

cleanup_old_logs() {
    echo "=== æ¸…ç†æ—§æ—¥å¿— ===" >> $REPORT_FILE
    
    # åˆ é™¤30å¤©å‰çš„æ—¥å¿—æ–‡ä»¶
    deleted_files=$(find $LOG_DIR -name "*.log.*" -mtime +30 -delete -print | wc -l)
    echo "å·²åˆ é™¤ $deleted_files ä¸ªæ—§æ—¥å¿—æ–‡ä»¶" >> $REPORT_FILE
    
    # å‹ç¼©7å¤©å‰çš„æ—¥å¿—æ–‡ä»¶
    find $LOG_DIR -name "*.log" -mtime +7 ! -name "*.gz" -exec gzip {} \;
    compressed_files=$(find $LOG_DIR -name "*.log.gz" -mtime -1 | wc -l)
    echo "å·²å‹ç¼© $compressed_files ä¸ªæ—¥å¿—æ–‡ä»¶" >> $REPORT_FILE
}

# æ‰§è¡Œåˆ†æ
analyze_error_patterns
analyze_performance
check_disk_usage
cleanup_old_logs

# å‘é€æŠ¥å‘Š
echo "æ—¥å¿—åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: $REPORT_FILE"
cat $REPORT_FILE

# å¦‚æœæœ‰ä¸¥é‡é”™è¯¯ï¼Œå‘é€å‘Šè­¦
error_count=$(grep -c "ERROR" $LOG_DIR/jjcrawler-error.log 2>/dev/null || echo 0)
if [ $error_count -gt 10 ]; then
    echo "æ£€æµ‹åˆ°å¤§é‡é”™è¯¯ ($error_count ä¸ª)ï¼Œè¯·ç«‹å³æ£€æŸ¥" | \
    mail -s "JJCrawler3 é”™è¯¯å‘Šè­¦" admin@example.com
fi
```

## æ•°æ®å¤‡ä»½ä¸æ¢å¤

### 1. æ•°æ®å¤‡ä»½ç­–ç•¥

#### è‡ªåŠ¨å¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# backup.sh - æ•°æ®å¤‡ä»½è„šæœ¬

# é…ç½®å‚æ•°
BACKUP_DIR="/backup/jjcrawler"
DATA_DIR="/app/data"
RETENTION_DAYS=30
REMOTE_BACKUP_HOST="backup-server.example.com"
REMOTE_BACKUP_USER="backup"

# åˆ›å»ºå¤‡ä»½ç›®å½•
BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
CURRENT_BACKUP_DIR="$BACKUP_DIR/$BACKUP_DATE"
mkdir -p "$CURRENT_BACKUP_DIR"

# æ•°æ®åº“å¤‡ä»½
backup_database() {
    echo "å¼€å§‹å¤‡ä»½æ•°æ®åº“..."
    
    # SQLite æ•°æ®åº“å¤‡ä»½
    if [ -f "$DATA_DIR/jjcrawler.db" ]; then
        # åˆ›å»ºæ•°æ®åº“å¤‡ä»½
        sqlite3 "$DATA_DIR/jjcrawler.db" ".backup $CURRENT_BACKUP_DIR/jjcrawler.db"
        
        # å¯¼å‡º SQL æ–‡ä»¶
        sqlite3 "$DATA_DIR/jjcrawler.db" ".dump" > "$CURRENT_BACKUP_DIR/jjcrawler.sql"
        
        # éªŒè¯å¤‡ä»½å®Œæ•´æ€§
        if sqlite3 "$CURRENT_BACKUP_DIR/jjcrawler.db" "PRAGMA integrity_check;" | grep -q "ok"; then
            echo "âœ… æ•°æ®åº“å¤‡ä»½æˆåŠŸ"
        else
            echo "âŒ æ•°æ®åº“å¤‡ä»½éªŒè¯å¤±è´¥"
            return 1
        fi
    fi
}

# é…ç½®æ–‡ä»¶å¤‡ä»½
backup_config() {
    echo "å¤‡ä»½é…ç½®æ–‡ä»¶..."
    
    cp -r "$DATA_DIR/urls.json" "$CURRENT_BACKUP_DIR/"
    cp -r "$DATA_DIR/tasks/" "$CURRENT_BACKUP_DIR/"
    
    # å¤‡ä»½åº”ç”¨é…ç½®
    if [ -f "/app/.env" ]; then
        cp "/app/.env" "$CURRENT_BACKUP_DIR/"
    fi
    
    echo "âœ… é…ç½®æ–‡ä»¶å¤‡ä»½å®Œæˆ"
}

# æ—¥å¿—å¤‡ä»½
backup_logs() {
    echo "å¤‡ä»½å…³é”®æ—¥å¿—..."
    
    # åªå¤‡ä»½æœ€è¿‘7å¤©çš„æ—¥å¿—
    find /app/logs -name "*.log" -mtime -7 -exec cp {} "$CURRENT_BACKUP_DIR/" \;
    
    echo "âœ… æ—¥å¿—å¤‡ä»½å®Œæˆ"
}

# å‹ç¼©å¤‡ä»½
compress_backup() {
    echo "å‹ç¼©å¤‡ä»½æ–‡ä»¶..."
    
    cd "$BACKUP_DIR"
    tar -czf "${BACKUP_DATE}.tar.gz" "$BACKUP_DATE/"
    
    if [ $? -eq 0 ]; then
        rm -rf "$CURRENT_BACKUP_DIR"
        echo "âœ… å¤‡ä»½å‹ç¼©å®Œæˆ: ${BACKUP_DATE}.tar.gz"
    else
        echo "âŒ å¤‡ä»½å‹ç¼©å¤±è´¥"
        return 1
    fi
}

# æ¸…ç†æ—§å¤‡ä»½
cleanup_old_backups() {
    echo "æ¸…ç†æ—§å¤‡ä»½æ–‡ä»¶..."
    
    find "$BACKUP_DIR" -name "*.tar.gz" -mtime +$RETENTION_DAYS -delete
    
    local remaining_backups=$(find "$BACKUP_DIR" -name "*.tar.gz" | wc -l)
    echo "âœ… æ¸…ç†å®Œæˆï¼Œä¿ç•™ $remaining_backups ä¸ªå¤‡ä»½æ–‡ä»¶"
}

# è¿œç¨‹å¤‡ä»½åŒæ­¥
sync_remote_backup() {
    echo "åŒæ­¥åˆ°è¿œç¨‹å¤‡ä»½æœåŠ¡å™¨..."
    
    rsync -avz --delete "$BACKUP_DIR/" \
          "$REMOTE_BACKUP_USER@$REMOTE_BACKUP_HOST:/backup/jjcrawler/"
    
    if [ $? -eq 0 ]; then
        echo "âœ… è¿œç¨‹å¤‡ä»½åŒæ­¥å®Œæˆ"
    else
        echo "âš ï¸  è¿œç¨‹å¤‡ä»½åŒæ­¥å¤±è´¥"
    fi
}

# å¤‡ä»½éªŒè¯
verify_backup() {
    local backup_file="$BACKUP_DIR/${BACKUP_DATE}.tar.gz"
    
    echo "éªŒè¯å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§..."
    
    if tar -tzf "$backup_file" > /dev/null 2>&1; then
        echo "âœ… å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§éªŒè¯é€šè¿‡"
        
        # è·å–å¤‡ä»½å¤§å°
        local backup_size=$(du -h "$backup_file" | cut -f1)
        echo "å¤‡ä»½æ–‡ä»¶å¤§å°: $backup_size"
        
        return 0
    else
        echo "âŒ å¤‡ä»½æ–‡ä»¶æŸå"
        return 1
    fi
}

# å‘é€å¤‡ä»½æŠ¥å‘Š
send_backup_report() {
    local status=$1
    local report_file="/tmp/backup_report_$(date +%Y%m%d).txt"
    
    {
        echo "JJCrawler3 å¤‡ä»½æŠ¥å‘Š"
        echo "====================="
        echo "å¤‡ä»½æ—¶é—´: $(date)"
        echo "å¤‡ä»½çŠ¶æ€: $status"
        echo "å¤‡ä»½ä½ç½®: $BACKUP_DIR"
        echo ""
        echo "å¤‡ä»½å†…å®¹:"
        echo "- æ•°æ®åº“æ–‡ä»¶"
        echo "- é…ç½®æ–‡ä»¶"
        echo "- ä»»åŠ¡è®°å½•"
        echo "- å…³é”®æ—¥å¿—"
        echo ""
        echo "ç£ç›˜ä½¿ç”¨æƒ…å†µ:"
        df -h "$BACKUP_DIR"
    } > "$report_file"
    
    # å‘é€é‚®ä»¶æŠ¥å‘Š
    mail -s "JJCrawler3 å¤‡ä»½æŠ¥å‘Š - $status" admin@example.com < "$report_file"
}

# ä¸»å¤‡ä»½æµç¨‹
main() {
    echo "å¼€å§‹ JJCrawler3 æ•°æ®å¤‡ä»½..."
    echo "å¤‡ä»½æ—¶é—´: $(date)"
    echo "å¤‡ä»½ç›®å½•: $CURRENT_BACKUP_DIR"
    
    # åœæ­¢åº”ç”¨å†™å…¥ (å¯é€‰)
    # docker-compose pause jjcrawler
    
    if backup_database && backup_config && backup_logs; then
        if compress_backup && verify_backup; then
            cleanup_old_backups
            sync_remote_backup
            send_backup_report "æˆåŠŸ"
            echo "âœ… å¤‡ä»½æµç¨‹å®Œæˆ"
        else
            send_backup_report "å¤±è´¥ - å‹ç¼©æˆ–éªŒè¯é”™è¯¯"
            exit 1
        fi
    else
        send_backup_report "å¤±è´¥ - æ•°æ®å¤‡ä»½é”™è¯¯"
        exit 1
    fi
    
    # æ¢å¤åº”ç”¨ (å¯é€‰)
    # docker-compose unpause jjcrawler
}

# æ‰§è¡Œå¤‡ä»½
main
```

### 2. æ•°æ®æ¢å¤æµç¨‹

#### æ•°æ®æ¢å¤è„šæœ¬

```bash
#!/bin/bash
# restore.sh - æ•°æ®æ¢å¤è„šæœ¬

BACKUP_DIR="/backup/jjcrawler"
DATA_DIR="/app/data"
RESTORE_LOG="/tmp/restore_$(date +%Y%m%d_%H%M%S).log"

# åˆ—å‡ºå¯ç”¨å¤‡ä»½
list_backups() {
    echo "å¯ç”¨å¤‡ä»½åˆ—è¡¨:"
    ls -lht "$BACKUP_DIR"/*.tar.gz | head -10
}

# æ¢å¤æ•°æ®åº“
restore_database() {
    local backup_path="$1"
    
    echo "æ¢å¤æ•°æ®åº“..." | tee -a $RESTORE_LOG
    
    # å¤‡ä»½å½“å‰æ•°æ®åº“
    if [ -f "$DATA_DIR/jjcrawler.db" ]; then
        cp "$DATA_DIR/jjcrawler.db" "$DATA_DIR/jjcrawler.db.bak.$(date +%Y%m%d_%H%M%S)"
    fi
    
    # æ¢å¤æ•°æ®åº“æ–‡ä»¶
    if [ -f "$backup_path/jjcrawler.db" ]; then
        cp "$backup_path/jjcrawler.db" "$DATA_DIR/"
        echo "âœ… æ•°æ®åº“æ–‡ä»¶æ¢å¤å®Œæˆ" | tee -a $RESTORE_LOG
    else
        echo "âŒ å¤‡ä»½ä¸­æœªæ‰¾åˆ°æ•°æ®åº“æ–‡ä»¶" | tee -a $RESTORE_LOG
        return 1
    fi
    
    # éªŒè¯æ•°æ®åº“å®Œæ•´æ€§
    if sqlite3 "$DATA_DIR/jjcrawler.db" "PRAGMA integrity_check;" | grep -q "ok"; then
        echo "âœ… æ•°æ®åº“å®Œæ•´æ€§éªŒè¯é€šè¿‡" | tee -a $RESTORE_LOG
    else
        echo "âŒ æ•°æ®åº“å®Œæ•´æ€§éªŒè¯å¤±è´¥" | tee -a $RESTORE_LOG
        return 1
    fi
}

# æ¢å¤é…ç½®æ–‡ä»¶
restore_config() {
    local backup_path="$1"
    
    echo "æ¢å¤é…ç½®æ–‡ä»¶..." | tee -a $RESTORE_LOG
    
    # æ¢å¤ URLs é…ç½®
    if [ -f "$backup_path/urls.json" ]; then
        cp "$backup_path/urls.json" "$DATA_DIR/"
        echo "âœ… URLs é…ç½®æ¢å¤å®Œæˆ" | tee -a $RESTORE_LOG
    fi
    
    # æ¢å¤ä»»åŠ¡é…ç½®
    if [ -d "$backup_path/tasks" ]; then
        cp -r "$backup_path/tasks" "$DATA_DIR/"
        echo "âœ… ä»»åŠ¡é…ç½®æ¢å¤å®Œæˆ" | tee -a $RESTORE_LOG
    fi
    
    # æ¢å¤ç¯å¢ƒé…ç½®
    if [ -f "$backup_path/.env" ]; then
        cp "$backup_path/.env" "/app/"
        echo "âœ… ç¯å¢ƒé…ç½®æ¢å¤å®Œæˆ" | tee -a $RESTORE_LOG
    fi
}

# éªŒè¯æ¢å¤ç»“æœ
verify_restore() {
    echo "éªŒè¯æ¢å¤ç»“æœ..." | tee -a $RESTORE_LOG
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    local files=("$DATA_DIR/jjcrawler.db" "$DATA_DIR/urls.json")
    
    for file in "${files[@]}"; do
        if [ -f "$file" ]; then
            echo "âœ… $file å­˜åœ¨" | tee -a $RESTORE_LOG
        else
            echo "âŒ $file ç¼ºå¤±" | tee -a $RESTORE_LOG
            return 1
        fi
    done
    
    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    if sqlite3 "$DATA_DIR/jjcrawler.db" "SELECT COUNT(*) FROM books;" > /dev/null 2>&1; then
        echo "âœ… æ•°æ®åº“è¿æ¥æµ‹è¯•é€šè¿‡" | tee -a $RESTORE_LOG
    else
        echo "âŒ æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥" | tee -a $RESTORE_LOG
        return 1
    fi
}

# ä¸»æ¢å¤æµç¨‹
restore_from_backup() {
    local backup_file="$1"
    
    if [ ! -f "$backup_file" ]; then
        echo "é”™è¯¯: å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: $backup_file"
        return 1
    fi
    
    echo "å¼€å§‹ä»å¤‡ä»½æ¢å¤: $backup_file" | tee -a $RESTORE_LOG
    
    # åœæ­¢åº”ç”¨
    echo "åœæ­¢åº”ç”¨æœåŠ¡..." | tee -a $RESTORE_LOG
    docker-compose stop jjcrawler
    
    # è§£å‹å¤‡ä»½
    local temp_dir="/tmp/restore_$(date +%Y%m%d_%H%M%S)"
    mkdir -p "$temp_dir"
    
    echo "è§£å‹å¤‡ä»½æ–‡ä»¶..." | tee -a $RESTORE_LOG
    tar -xzf "$backup_file" -C "$temp_dir"
    
    local backup_path="$temp_dir/$(basename "$backup_file" .tar.gz)"
    
    # æ‰§è¡Œæ¢å¤
    if restore_database "$backup_path" && restore_config "$backup_path"; then
        if verify_restore; then
            echo "âœ… æ•°æ®æ¢å¤å®Œæˆ" | tee -a $RESTORE_LOG
            
            # é‡å¯åº”ç”¨
            echo "é‡å¯åº”ç”¨æœåŠ¡..." | tee -a $RESTORE_LOG
            docker-compose start jjcrawler
            
            # ç­‰å¾…æœåŠ¡å¯åŠ¨
            sleep 30
            
            # éªŒè¯æœåŠ¡
            if curl -s -f http://localhost:8000/health > /dev/null; then
                echo "âœ… æœåŠ¡æ¢å¤æˆåŠŸ" | tee -a $RESTORE_LOG
            else
                echo "âš ï¸  æœåŠ¡å¯åŠ¨å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥" | tee -a $RESTORE_LOG
            fi
        else
            echo "âŒ æ¢å¤éªŒè¯å¤±è´¥" | tee -a $RESTORE_LOG
            return 1
        fi
    else
        echo "âŒ æ•°æ®æ¢å¤å¤±è´¥" | tee -a $RESTORE_LOG
        return 1
    fi
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    rm -rf "$temp_dir"
}

# è„šæœ¬ä½¿ç”¨è¯´æ˜
usage() {
    echo "ç”¨æ³•: $0 [é€‰é¡¹]"
    echo "é€‰é¡¹:"
    echo "  -l, --list              åˆ—å‡ºå¯ç”¨å¤‡ä»½"
    echo "  -r, --restore <file>    ä»æŒ‡å®šå¤‡ä»½æ¢å¤"
    echo "  -h, --help              æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"
}

# ä¸»å‡½æ•°
main() {
    case "$1" in
        -l|--list)
            list_backups
            ;;
        -r|--restore)
            if [ -z "$2" ]; then
                echo "é”™è¯¯: è¯·æŒ‡å®šå¤‡ä»½æ–‡ä»¶"
                usage
                exit 1
            fi
            restore_from_backup "$2"
            ;;
        -h|--help)
            usage
            ;;
        *)
            usage
            exit 1
            ;;
    esac
}

main "$@"
```

## ç³»ç»Ÿç»´æŠ¤

### 1. å®šæœŸç»´æŠ¤ä»»åŠ¡

#### æ•°æ®åº“ç»´æŠ¤

```bash
#!/bin/bash
# db_maintenance.sh - æ•°æ®åº“ç»´æŠ¤è„šæœ¬

DB_PATH="/app/data/jjcrawler.db"
MAINTENANCE_LOG="/var/log/jjcrawler/maintenance.log"

# æ•°æ®åº“ä¼˜åŒ–
optimize_database() {
    echo "æ‰§è¡Œæ•°æ®åº“ä¼˜åŒ–..." | tee -a $MAINTENANCE_LOG
    
    # VACUUM æ“ä½œ
    sqlite3 "$DB_PATH" "VACUUM;" 2>&1 | tee -a $MAINTENANCE_LOG
    
    # é‡æ–°åˆ†æç»Ÿè®¡ä¿¡æ¯
    sqlite3 "$DB_PATH" "ANALYZE;" 2>&1 | tee -a $MAINTENANCE_LOG
    
    # æ£€æŸ¥å®Œæ•´æ€§
    local integrity_check=$(sqlite3 "$DB_PATH" "PRAGMA integrity_check;")
    if [ "$integrity_check" = "ok" ]; then
        echo "âœ… æ•°æ®åº“å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡" | tee -a $MAINTENANCE_LOG
    else
        echo "âŒ æ•°æ®åº“å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: $integrity_check" | tee -a $MAINTENANCE_LOG
        return 1
    fi
    
    # è·å–æ•°æ®åº“å¤§å°
    local db_size=$(du -h "$DB_PATH" | cut -f1)
    echo "æ•°æ®åº“å½“å‰å¤§å°: $db_size" | tee -a $MAINTENANCE_LOG
}

# æ¸…ç†è¿‡æœŸæ•°æ®
cleanup_expired_data() {
    echo "æ¸…ç†è¿‡æœŸæ•°æ®..." | tee -a $MAINTENANCE_LOG
    
    # æ¸…ç†90å¤©å‰çš„å¿«ç…§æ•°æ®
    local deleted_snapshots=$(sqlite3 "$DB_PATH" "
        DELETE FROM book_snapshots 
        WHERE created_at < datetime('now', '-90 days');
        SELECT changes();
    ")
    
    echo "å·²æ¸…ç† $deleted_snapshots æ¡è¿‡æœŸä¹¦ç±å¿«ç…§" | tee -a $MAINTENANCE_LOG
    
    # æ¸…ç†90å¤©å‰çš„æ’è¡Œæ¦œå¿«ç…§
    local deleted_rankings=$(sqlite3 "$DB_PATH" "
        DELETE FROM ranking_snapshots 
        WHERE created_at < datetime('now', '-90 days');
        SELECT changes();
    ")
    
    echo "å·²æ¸…ç† $deleted_rankings æ¡è¿‡æœŸæ’è¡Œæ¦œå¿«ç…§" | tee -a $MAINTENANCE_LOG
}

# æ›´æ–°æ•°æ®åº“ç»Ÿè®¡
update_statistics() {
    echo "æ›´æ–°æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯..." | tee -a $MAINTENANCE_LOG
    
    local total_books=$(sqlite3 "$DB_PATH" "SELECT COUNT(*) FROM books;")
    local total_snapshots=$(sqlite3 "$DB_PATH" "SELECT COUNT(*) FROM book_snapshots;")
    local total_rankings=$(sqlite3 "$DB_PATH" "SELECT COUNT(*) FROM ranking_snapshots;")
    
    echo "æ•°æ®ç»Ÿè®¡:" | tee -a $MAINTENANCE_LOG
    echo "- æ€»ä¹¦ç±æ•°: $total_books" | tee -a $MAINTENANCE_LOG
    echo "- æ€»å¿«ç…§æ•°: $total_snapshots" | tee -a $MAINTENANCE_LOG
    echo "- æ€»æ’è¡Œæ¦œå¿«ç…§æ•°: $total_rankings" | tee -a $MAINTENANCE_LOG
}

# ä¸»ç»´æŠ¤æµç¨‹
main() {
    echo "å¼€å§‹æ•°æ®åº“ç»´æŠ¤ - $(date)" | tee -a $MAINTENANCE_LOG
    
    # åœæ­¢å†™å…¥æ“ä½œ (å¯é€‰)
    # docker-compose exec jjcrawler curl -X POST http://localhost:8000/api/v1/maintenance/enable
    
    if optimize_database && cleanup_expired_data; then
        update_statistics
        echo "âœ… æ•°æ®åº“ç»´æŠ¤å®Œæˆ" | tee -a $MAINTENANCE_LOG
    else
        echo "âŒ æ•°æ®åº“ç»´æŠ¤å¤±è´¥" | tee -a $MAINTENANCE_LOG
        exit 1
    fi
    
    # æ¢å¤å†™å…¥æ“ä½œ (å¯é€‰)
    # docker-compose exec jjcrawler curl -X POST http://localhost:8000/api/v1/maintenance/disable
}

main
```

#### ç³»ç»Ÿæ¸…ç†è„šæœ¬

```bash
#!/bin/bash
# system_cleanup.sh - ç³»ç»Ÿæ¸…ç†è„šæœ¬

# Docker æ¸…ç†
cleanup_docker() {
    echo "æ¸…ç† Docker èµ„æº..."
    
    # æ¸…ç†æœªä½¿ç”¨çš„é•œåƒ
    docker image prune -f
    
    # æ¸…ç†æœªä½¿ç”¨çš„å®¹å™¨
    docker container prune -f
    
    # æ¸…ç†æœªä½¿ç”¨çš„ç½‘ç»œ
    docker network prune -f
    
    # æ¸…ç†æœªä½¿ç”¨çš„å·
    docker volume prune -f
    
    echo "âœ… Docker æ¸…ç†å®Œæˆ"
}

# æ—¥å¿—æ¸…ç†
cleanup_logs() {
    echo "æ¸…ç†ç³»ç»Ÿæ—¥å¿—..."
    
    # æ¸…ç† journal æ—¥å¿—
    journalctl --vacuum-time=30d
    
    # æ¸…ç†åº”ç”¨æ—¥å¿—
    find /app/logs -name "*.log.*" -mtime +30 -delete
    
    echo "âœ… æ—¥å¿—æ¸…ç†å®Œæˆ"
}

# ä¸´æ—¶æ–‡ä»¶æ¸…ç†
cleanup_temp_files() {
    echo "æ¸…ç†ä¸´æ—¶æ–‡ä»¶..."
    
    # æ¸…ç†ç³»ç»Ÿä¸´æ—¶æ–‡ä»¶
    find /tmp -type f -mtime +7 -delete 2>/dev/null
    
    # æ¸…ç†åº”ç”¨ä¸´æ—¶æ–‡ä»¶
    find /app -name "*.tmp" -mtime +1 -delete 2>/dev/null
    
    echo "âœ… ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ"
}

# ä¸»æ¸…ç†æµç¨‹
main() {
    echo "å¼€å§‹ç³»ç»Ÿæ¸…ç† - $(date)"
    
    cleanup_docker
    cleanup_logs
    cleanup_temp_files
    
    echo "âœ… ç³»ç»Ÿæ¸…ç†å®Œæˆ"
    
    # æ˜¾ç¤ºæ¸…ç†åçš„ç£ç›˜ä½¿ç”¨æƒ…å†µ
    echo "ç£ç›˜ä½¿ç”¨æƒ…å†µ:"
    df -h
}

main
```

### 2. ç³»ç»Ÿæ›´æ–°å’Œå‡çº§

#### åº”ç”¨å‡çº§è„šæœ¬

```bash
#!/bin/bash
# upgrade.sh - åº”ç”¨å‡çº§è„šæœ¬

# é…ç½®å‚æ•°
BACKUP_DIR="/backup/jjcrawler/upgrades"
SERVICE_NAME="jjcrawler"
HEALTH_CHECK_URL="http://localhost:8000/health"
ROLLBACK_TIMEOUT=300

# é¢„å‡çº§æ£€æŸ¥
pre_upgrade_check() {
    echo "æ‰§è¡Œé¢„å‡çº§æ£€æŸ¥..."
    
    # æ£€æŸ¥å½“å‰æœåŠ¡çŠ¶æ€
    if ! curl -s -f "$HEALTH_CHECK_URL" > /dev/null; then
        echo "âŒ å½“å‰æœåŠ¡çŠ¶æ€å¼‚å¸¸ï¼Œå‡çº§ä¸­æ­¢"
        return 1
    fi
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    local disk_usage=$(df /app | awk 'NR==2 {print $5}' | sed 's/%//')
    if [ $disk_usage -gt 80 ]; then
        echo "âŒ ç£ç›˜ç©ºé—´ä¸è¶³ ($disk_usage%)ï¼Œå‡çº§ä¸­æ­¢"
        return 1
    fi
    
    # æ£€æŸ¥å†…å­˜ä½¿ç”¨
    local mem_usage=$(free | awk 'NR==2{printf "%.0f", $3*100/$2}')
    if [ $mem_usage -gt 90 ]; then
        echo "âŒ å†…å­˜ä½¿ç”¨è¿‡é«˜ ($mem_usage%)ï¼Œå‡çº§ä¸­æ­¢"
        return 1
    fi
    
    echo "âœ… é¢„å‡çº§æ£€æŸ¥é€šè¿‡"
}

# åˆ›å»ºå‡çº§å¤‡ä»½
create_upgrade_backup() {
    echo "åˆ›å»ºå‡çº§å¤‡ä»½..."
    
    local backup_path="$BACKUP_DIR/upgrade_$(date +%Y%m%d_%H%M%S)"
    mkdir -p "$backup_path"
    
    # å¤‡ä»½å½“å‰ç‰ˆæœ¬
    docker save jjcrawler:latest > "$backup_path/jjcrawler_current.tar"
    
    # å¤‡ä»½æ•°æ®å’Œé…ç½®
    cp -r /app/data "$backup_path/"
    cp /app/.env "$backup_path/" 2>/dev/null || true
    cp docker-compose.yml "$backup_path/"
    
    echo "upgrade_backup_path=$backup_path" > /tmp/upgrade_vars.env
    echo "âœ… å‡çº§å¤‡ä»½åˆ›å»ºå®Œæˆ: $backup_path"
}

# æ‰§è¡Œå‡çº§
perform_upgrade() {
    echo "å¼€å§‹æ‰§è¡Œå‡çº§..."
    
    # æ‹‰å–æ–°é•œåƒ
    echo "æ‹‰å–æ–°é•œåƒ..."
    docker-compose pull
    
    # åœæ­¢å½“å‰æœåŠ¡
    echo "åœæ­¢å½“å‰æœåŠ¡..."
    docker-compose down
    
    # å¯åŠ¨æ–°ç‰ˆæœ¬
    echo "å¯åŠ¨æ–°ç‰ˆæœ¬..."
    docker-compose up -d
    
    echo "âœ… å‡çº§éƒ¨ç½²å®Œæˆ"
}

# å‡çº§åéªŒè¯
post_upgrade_verification() {
    echo "æ‰§è¡Œå‡çº§åéªŒè¯..."
    
    local retry_count=0
    local max_retries=30
    
    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    while [ $retry_count -lt $max_retries ]; do
        if curl -s -f "$HEALTH_CHECK_URL" > /dev/null; then
            echo "âœ… æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡"
            break
        else
            echo "ç­‰å¾…æœåŠ¡å¯åŠ¨... ($((retry_count + 1))/$max_retries)"
            sleep 10
            ((retry_count++))
        fi
    done
    
    if [ $retry_count -ge $max_retries ]; then
        echo "âŒ æœåŠ¡å¯åŠ¨è¶…æ—¶ï¼Œéœ€è¦å›æ»š"
        return 1
    fi
    
    # åŠŸèƒ½æµ‹è¯•
    echo "æ‰§è¡ŒåŠŸèƒ½æµ‹è¯•..."
    
    # æµ‹è¯• API ç«¯ç‚¹
    local test_endpoints=(
        "/health"
        "/api/v1/pages"
        "/stats"
    )
    
    for endpoint in "${test_endpoints[@]}"; do
        if curl -s -f "http://localhost:8000$endpoint" > /dev/null; then
            echo "âœ… $endpoint æµ‹è¯•é€šè¿‡"
        else
            echo "âŒ $endpoint æµ‹è¯•å¤±è´¥"
            return 1
        fi
    done
    
    echo "âœ… å‡çº§åéªŒè¯å®Œæˆ"
}

# å›æ»šæ“ä½œ
rollback() {
    echo "æ‰§è¡Œå›æ»šæ“ä½œ..."
    
    source /tmp/upgrade_vars.env 2>/dev/null || {
        echo "âŒ æ— æ³•æ‰¾åˆ°å¤‡ä»½è·¯å¾„ï¼Œå›æ»šå¤±è´¥"
        return 1
    }
    
    # åœæ­¢å½“å‰æœåŠ¡
    docker-compose down
    
    # æ¢å¤é•œåƒ
    if [ -f "$upgrade_backup_path/jjcrawler_current.tar" ]; then
        docker load < "$upgrade_backup_path/jjcrawler_current.tar"
    fi
    
    # æ¢å¤é…ç½®å’Œæ•°æ®
    cp -r "$upgrade_backup_path/data"/* /app/data/
    cp "$upgrade_backup_path/.env" /app/ 2>/dev/null || true
    cp "$upgrade_backup_path/docker-compose.yml" ./
    
    # å¯åŠ¨æœåŠ¡
    docker-compose up -d
    
    # éªŒè¯å›æ»š
    sleep 30
    if curl -s -f "$HEALTH_CHECK_URL" > /dev/null; then
        echo "âœ… å›æ»šæˆåŠŸ"
    else
        echo "âŒ å›æ»šéªŒè¯å¤±è´¥"
        return 1
    fi
}

# ä¸»å‡çº§æµç¨‹
main() {
    echo "å¼€å§‹ JJCrawler3 å‡çº§æµç¨‹..."
    echo "å‡çº§æ—¶é—´: $(date)"
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    local upgrade_log="/var/log/jjcrawler/upgrade_$(date +%Y%m%d_%H%M%S).log"
    exec > >(tee -a "$upgrade_log") 2>&1
    
    if pre_upgrade_check && create_upgrade_backup; then
        if perform_upgrade; then
            if post_upgrade_verification; then
                echo "âœ… å‡çº§æˆåŠŸå®Œæˆ"
                
                # å‘é€æˆåŠŸé€šçŸ¥
                echo "JJCrawler3 å‡çº§æˆåŠŸå®Œæˆ" | \
                mail -s "å‡çº§æˆåŠŸé€šçŸ¥" admin@example.com
                
                # æ¸…ç†æ—§å¤‡ä»½ (ä¿ç•™æœ€è¿‘5æ¬¡)
                find "$BACKUP_DIR" -maxdepth 1 -type d -name "upgrade_*" | \
                sort -r | tail -n +6 | xargs rm -rf
                
            else
                echo "âŒ å‡çº§åéªŒè¯å¤±è´¥ï¼Œæ‰§è¡Œå›æ»š"
                if rollback; then
                    echo "âœ… å›æ»šæˆåŠŸ"
                else
                    echo "âŒ å›æ»šå¤±è´¥ï¼Œéœ€è¦äººå·¥å¹²é¢„"
                    exit 1
                fi
            fi
        else
            echo "âŒ å‡çº§éƒ¨ç½²å¤±è´¥ï¼Œæ‰§è¡Œå›æ»š"
            rollback
        fi
    else
        echo "âŒ é¢„å‡çº§æ£€æŸ¥æˆ–å¤‡ä»½å¤±è´¥ï¼Œå‡çº§ä¸­æ­¢"
        exit 1
    fi
}

# å¤„ç†å‘½ä»¤è¡Œå‚æ•°
case "$1" in
    --check)
        pre_upgrade_check
        ;;
    --rollback)
        rollback
        ;;
    *)
        main
        ;;
esac
```

## æ€§èƒ½ä¼˜åŒ–

### 1. åº”ç”¨æ€§èƒ½ä¼˜åŒ–

#### æ•°æ®åº“æ€§èƒ½è°ƒä¼˜

```python
# app/modules/database/optimization.py
import sqlite3
from typing import List, Dict
from app.utils.log_utils import get_logger

logger = get_logger(__name__)

class DatabaseOptimizer:
    """æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
    
    def optimize_sqlite_settings(self):
        """ä¼˜åŒ– SQLite è®¾ç½®"""
        with sqlite3.connect(self.db_path) as conn:
            # å¯ç”¨ WAL æ¨¡å¼
            conn.execute("PRAGMA journal_mode = WAL")
            
            # å¢åŠ ç¼“å­˜å¤§å°
            conn.execute("PRAGMA cache_size = -65536")  # 64MB
            
            # è®¾ç½®åŒæ­¥æ¨¡å¼
            conn.execute("PRAGMA synchronous = NORMAL")
            
            # å¯ç”¨å¤–é”®çº¦æŸ
            conn.execute("PRAGMA foreign_keys = ON")
            
            # è®¾ç½®å†…å­˜ä¸´æ—¶å­˜å‚¨
            conn.execute("PRAGMA temp_store = MEMORY")
            
            # ä¼˜åŒ–é¡µé¢å¤§å°
            conn.execute("PRAGMA page_size = 4096")
            
            logger.info("SQLite æ€§èƒ½ä¼˜åŒ–è®¾ç½®å·²åº”ç”¨")
    
    def create_optimized_indexes(self):
        """åˆ›å»ºä¼˜åŒ–ç´¢å¼•"""
        indexes = [
            # ä¹¦ç±è¡¨ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_books_novel_id ON books(novel_id)",
            "CREATE INDEX IF NOT EXISTS idx_books_title ON books(title)",
            "CREATE INDEX IF NOT EXISTS idx_books_author ON books(author)",
            "CREATE INDEX IF NOT EXISTS idx_books_status ON books(status)",
            
            # ä¹¦ç±å¿«ç…§è¡¨ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_book_snapshots_book_id ON book_snapshots(book_id)",
            "CREATE INDEX IF NOT EXISTS idx_book_snapshots_created_at ON book_snapshots(created_at)",
            "CREATE INDEX IF NOT EXISTS idx_book_snapshots_book_created ON book_snapshots(book_id, created_at)",
            
            # æ’è¡Œæ¦œå¿«ç…§è¡¨ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_ranking_snapshots_ranking_id ON ranking_snapshots(ranking_id)",
            "CREATE INDEX IF NOT EXISTS idx_ranking_snapshots_created_at ON ranking_snapshots(created_at)",
            "CREATE INDEX IF NOT EXISTS idx_ranking_snapshots_ranking_created ON ranking_snapshots(ranking_id, created_at)",
            
            # å¤åˆç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_books_compound ON books(status, genre, updated_at)",
        ]
        
        with sqlite3.connect(self.db_path) as conn:
            for index_sql in indexes:
                try:
                    conn.execute(index_sql)
                    logger.debug(f"åˆ›å»ºç´¢å¼•: {index_sql}")
                except sqlite3.Error as e:
                    logger.warning(f"ç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
        
        logger.info("æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–å®Œæˆ")
    
    def analyze_query_performance(self) -> Dict:
        """åˆ†ææŸ¥è¯¢æ€§èƒ½"""
        with sqlite3.connect(self.db_path) as conn:
            # å¯ç”¨æŸ¥è¯¢è®¡åˆ’åˆ†æ
            conn.execute("PRAGMA optimize")
            
            # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
            conn.execute("ANALYZE")
            
            # è·å–æ•°æ®åº“ç»Ÿè®¡
            cursor = conn.execute("""
                SELECT 
                    name as table_name,
                    COUNT(*) as row_count
                FROM sqlite_master 
                WHERE type='table' AND name NOT LIKE 'sqlite_%'
            """)
            
            table_stats = {row[0]: row[1] for row in cursor.fetchall()}
            
            # è·å–ç´¢å¼•ä½¿ç”¨æƒ…å†µ
            cursor = conn.execute("""
                SELECT name as index_name, tbl_name as table_name 
                FROM sqlite_master 
                WHERE type='index' AND name NOT LIKE 'sqlite_%'
            """)
            
            index_stats = [{"index": row[0], "table": row[1]} for row in cursor.fetchall()]
            
            return {
                "table_stats": table_stats,
                "index_stats": index_stats,
                "total_tables": len(table_stats),
                "total_indexes": len(index_stats)
            }
```

#### çˆ¬è™«æ€§èƒ½ä¼˜åŒ–

```python
# app/modules/crawler/performance.py
import asyncio
import time
from typing import List, Dict, Optional
from app.utils.log_utils import get_logger

logger = get_logger(__name__)

class CrawlerPerformanceOptimizer:
    """çˆ¬è™«æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.request_stats = []
        self.error_stats = []
    
    async def optimized_batch_crawl(self, urls: List[str], batch_size: int = 5) -> List[Dict]:
        """ä¼˜åŒ–çš„æ‰¹é‡çˆ¬å–"""
        results = []
        
        # å°† URLs åˆ†æ‰¹å¤„ç†
        for i in range(0, len(urls), batch_size):
            batch = urls[i:i + batch_size]
            
            # å¹¶å‘æ‰§è¡Œæ‰¹æ¬¡
            batch_results = await asyncio.gather(
                *[self._crawl_single_url(url) for url in batch],
                return_exceptions=True
            )
            
            # å¤„ç†ç»“æœ
            for url, result in zip(batch, batch_results):
                if isinstance(result, Exception):
                    logger.error(f"çˆ¬å–å¤±è´¥ {url}: {result}")
                    self.error_stats.append({"url": url, "error": str(result)})
                else:
                    results.append(result)
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if i + batch_size < len(urls):
                await asyncio.sleep(1)
        
        return results
    
    async def _crawl_single_url(self, url: str) -> Dict:
        """å•ä¸ª URL çˆ¬å–"""
        start_time = time.time()
        
        try:
            # å®é™…çˆ¬å–é€»è¾‘
            result = await self._perform_crawl(url)
            
            # è®°å½•æ€§èƒ½ç»Ÿè®¡
            duration = time.time() - start_time
            self.request_stats.append({
                "url": url,
                "duration": duration,
                "success": True
            })
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            self.request_stats.append({
                "url": url,
                "duration": duration,
                "success": False,
                "error": str(e)
            })
            raise
    
    def get_performance_report(self) -> Dict:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        if not self.request_stats:
            return {"message": "æ— æ€§èƒ½æ•°æ®"}
        
        successful_requests = [r for r in self.request_stats if r["success"]]
        failed_requests = [r for r in self.request_stats if not r["success"]]
        
        if successful_requests:
            durations = [r["duration"] for r in successful_requests]
            avg_duration = sum(durations) / len(durations)
            max_duration = max(durations)
            min_duration = min(durations)
        else:
            avg_duration = max_duration = min_duration = 0
        
        return {
            "total_requests": len(self.request_stats),
            "successful_requests": len(successful_requests),
            "failed_requests": len(failed_requests),
            "success_rate": len(successful_requests) / len(self.request_stats) * 100,
            "avg_duration": avg_duration,
            "max_duration": max_duration,
            "min_duration": min_duration,
            "errors": self.error_stats[-10:]  # æœ€è¿‘10ä¸ªé”™è¯¯
        }
```

### 2. ç³»ç»Ÿæ€§èƒ½ç›‘æ§

#### æ€§èƒ½ç›‘æ§è„šæœ¬

```bash
#!/bin/bash
# performance_monitor.sh - æ€§èƒ½ç›‘æ§è„šæœ¬

MONITOR_LOG="/var/log/jjcrawler/performance.log"
ALERT_THRESHOLD_CPU=80
ALERT_THRESHOLD_MEMORY=85
ALERT_THRESHOLD_DISK=90

# æ”¶é›†ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
collect_system_metrics() {
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    # CPU ä½¿ç”¨ç‡
    local cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | sed 's/%us,//')
    
    # å†…å­˜ä½¿ç”¨ç‡
    local memory_usage=$(free | awk 'NR==2{printf "%.1f", $3*100/$2}')
    
    # ç£ç›˜ä½¿ç”¨ç‡
    local disk_usage=$(df -h /app | awk 'NR==2 {print $5}' | sed 's/%//')
    
    # ç½‘ç»œè¿æ¥æ•°
    local tcp_connections=$(netstat -an | grep ESTABLISHED | wc -l)
    
    # Docker å®¹å™¨ç»Ÿè®¡
    local container_stats=$(docker stats --no-stream --format "{{.CPUPerc}},{{.MemUsage}}" jjcrawler 2>/dev/null)
    
    # è®°å½•æŒ‡æ ‡
    echo "[$timestamp] CPU:${cpu_usage}% MEM:${memory_usage}% DISK:${disk_usage}% TCP:${tcp_connections} CONTAINER:${container_stats}" >> $MONITOR_LOG
    
    # æ£€æŸ¥å‘Šè­¦é˜ˆå€¼
    check_performance_alerts "$cpu_usage" "$memory_usage" "$disk_usage"
}

# æ£€æŸ¥æ€§èƒ½å‘Šè­¦
check_performance_alerts() {
    local cpu=$1
    local memory=$2
    local disk=$3
    
    local alerts=()
    
    if (( $(echo "$cpu > $ALERT_THRESHOLD_CPU" | bc -l) )); then
        alerts+=("CPUä½¿ç”¨ç‡è¿‡é«˜: ${cpu}%")
    fi
    
    if (( $(echo "$memory > $ALERT_THRESHOLD_MEMORY" | bc -l) )); then
        alerts+=("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: ${memory}%")
    fi
    
    if [ "$disk" -gt "$ALERT_THRESHOLD_DISK" ]; then
        alerts+=("ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: ${disk}%")
    fi
    
    # å‘é€å‘Šè­¦
    if [ ${#alerts[@]} -gt 0 ]; then
        local alert_message="JJCrawler3 æ€§èƒ½å‘Šè­¦:\n$(printf '%s\n' "${alerts[@]}")"
        echo -e "$alert_message" | mail -s "æ€§èƒ½å‘Šè­¦" admin@example.com
    fi
}

# åº”ç”¨æ€§èƒ½åˆ†æ
analyze_application_performance() {
    local response_time=$(curl -w "%{time_total}" -s -o /dev/null http://localhost:8000/health)
    local status_code=$(curl -w "%{http_code}" -s -o /dev/null http://localhost:8000/health)
    
    echo "APIå“åº”æ—¶é—´: ${response_time}s, çŠ¶æ€ç : $status_code" >> $MONITOR_LOG
    
    # æ£€æŸ¥å“åº”æ—¶é—´å‘Šè­¦
    if (( $(echo "$response_time > 5" | bc -l) )); then
        echo "APIå“åº”æ—¶é—´è¿‡é•¿: ${response_time}s" | \
        mail -s "APIæ€§èƒ½å‘Šè­¦" admin@example.com
    fi
}

# ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
generate_performance_report() {
    local report_file="/tmp/performance_report_$(date +%Y%m%d).txt"
    
    {
        echo "JJCrawler3 æ€§èƒ½æŠ¥å‘Š"
        echo "==================="
        echo "æŠ¥å‘Šæ—¶é—´: $(date)"
        echo ""
        
        echo "ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ:"
        echo "---------------"
        echo "CPU: $(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | sed 's/%us,//')%"
        echo "å†…å­˜: $(free | awk 'NR==2{printf "%.1f", $3*100/$2}')%"
        echo "ç£ç›˜: $(df -h /app | awk 'NR==2 {print $5}')"
        echo ""
        
        echo "æœ€è¿‘24å°æ—¶æ€§èƒ½è¶‹åŠ¿:"
        echo "----------------"
        tail -n 288 $MONITOR_LOG | awk -F'[][]' '{print $2}' | \
        awk '{cpu+=$2; mem+=$3; disk+=$4; count++} END {
            printf "å¹³å‡CPU: %.1f%%\n", cpu/count
            printf "å¹³å‡å†…å­˜: %.1f%%\n", mem/count  
            printf "å¹³å‡ç£ç›˜: %.1f%%\n", disk/count
        }'
        echo ""
        
        echo "åº”ç”¨æ€§èƒ½æŒ‡æ ‡:"
        echo "----------"
        curl -s http://localhost:8000/api/v1/stats | jq -r '
            "ä»»åŠ¡æˆåŠŸç‡: " + (.scheduler_stats.total_succeeded / .scheduler_stats.total_executed * 100 | tostring) + "%",
            "æ´»è·ƒä»»åŠ¡æ•°: " + (.scheduler_stats.active_jobs | tostring),
            "æœ€åæ‰§è¡Œæ—¶é—´: " + (.scheduler_stats.last_execution // "æ— ")
        '
        
    } > "$report_file"
    
    echo "æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: $report_file"
}

# ä¸»ç›‘æ§æµç¨‹
main() {
    case "$1" in
        "collect")
            collect_system_metrics
            analyze_application_performance
            ;;
        "report")
            generate_performance_report
            ;;
        *)
            echo "ç”¨æ³•: $0 {collect|report}"
            exit 1
            ;;
    esac
}

main "$@"
```

é€šè¿‡è¿™ä¸ªå®Œæ•´çš„ç”Ÿäº§ç¯å¢ƒè¿ç»´æŒ‡å—ï¼Œæ‚¨å¯ä»¥æœ‰æ•ˆåœ°ç›‘æ§ã€ç»´æŠ¤å’Œä¼˜åŒ– JJCrawler3 ç³»ç»Ÿï¼Œç¡®ä¿å…¶åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„ç¨³å®šè¿è¡Œå’Œé«˜æ€§èƒ½è¡¨ç°ã€‚å»ºè®®æ ¹æ®å®é™…éƒ¨ç½²ç¯å¢ƒå’Œéœ€æ±‚ï¼Œè°ƒæ•´ç›¸å…³é…ç½®å‚æ•°å’Œç›‘æ§é˜ˆå€¼ã€‚