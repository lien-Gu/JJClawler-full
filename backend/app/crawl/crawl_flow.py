"""
çˆ¬å–æµç¨‹ç®¡ç†å™¨ - é«˜æ•ˆçš„å¹¶å‘çˆ¬å–æ¶æ„
"""

import asyncio
import json
import random
import time
from dataclasses import dataclass
from typing import Any, Dict, List
from typing import Tuple

import httpx
from sqlalchemy.orm import Session
from tenacity import retry, retry_if_exception, stop_after_attempt, \
    stop_after_delay, wait_exponential, wait_random

from app.config import get_settings
from app.crawl.crawl_task import PageTask, get_crawl_task
from app.crawl.http_client import HttpClient
from app.crawl.parser import NovelPageParser, PageParser, RankingParser
from app.database.connection import SessionLocal
from app.database.service.book_service import BookService
from app.database.service.ranking_service import RankingService
from app.logger import get_logger
from app.models.base import BaseResult
from app.utils import generate_batch_id

logger = get_logger(__name__)

# å…¨å±€503é”™è¯¯æš‚åœæ§åˆ¶æœºåˆ¶
_pause_duration = 10  # æš‚åœæ—¶é—´10ç§’
_pause_end_time = None  # æš‚åœç»“æŸæ—¶é—´
_pause_lock = None  # å…¨å±€æš‚åœé”


def get_pause_lock():
    """è·å–å…¨å±€æš‚åœé”"""
    global _pause_lock
    if _pause_lock is None:
        _pause_lock = asyncio.Lock()
    return _pause_lock


async def check_and_pause_if_needed():
    """æ£€æŸ¥å…¨å±€æš‚åœçŠ¶æ€ï¼Œå¦‚æœéœ€è¦åˆ™æš‚åœ"""
    global _pause_end_time
    
    # æ£€æŸ¥æ˜¯å¦åœ¨æš‚åœæœŸé—´
    current_time = time.time()
    if _pause_end_time and current_time < _pause_end_time:
        remaining_time = _pause_end_time - current_time
        logger.warning(f"â¸ï¸ å…¨å±€æš‚åœä¸­ï¼Œç­‰å¾… {remaining_time:.1f} ç§’åç»§ç»­")
        await asyncio.sleep(remaining_time)
        logger.info("âœ… æš‚åœç»“æŸï¼Œç»§ç»­çˆ¬å–")


async def mark_server_need_pause():
    """æ ‡è®°æœåŠ¡å™¨éœ€è¦æš‚åœ - 503é”™è¯¯è§¦å‘"""
    global _pause_end_time
    
    # ä½¿ç”¨é”ç¡®ä¿æš‚åœçŠ¶æ€çš„åŸå­æ€§è®¾ç½®
    async with get_pause_lock():
        current_time = time.time()
        # å¦‚æœå·²ç»åœ¨æš‚åœæœŸé—´ï¼Œä¸é‡å¤è®¾ç½®
        if _pause_end_time and current_time < _pause_end_time:
            return
            
        # è®¾ç½®æš‚åœç»“æŸæ—¶é—´
        _pause_end_time = current_time + _pause_duration
        logger.error(f"ğŸš¨ [503 ERROR] æ£€æµ‹åˆ°503é”™è¯¯ï¼Œè®¾ç½®å…¨å±€æš‚åœ {_pause_duration} ç§’ï¼Œ"f"æ‰€æœ‰åç»­è¯·æ±‚å°†ç­‰å¾…åˆ° {time.strftime('%H:%M:%S', time.localtime(_pause_end_time))}")




@dataclass
class PagesResult(BaseResult[PageParser]):
    """å¤šé¡µé¢çˆ¬å–ç»“æœ"""

    @property
    def rankings(self) -> List[RankingParser]:
        res = []
        for page in self.success_items:
            res.extend(page.rankings)
        return res

    def to_dict(self) -> Dict[str, Any]:
        return {
            "total_pages_num": self.total_num,
            "failed_pages": self.failed_ids
        }

    def get_novel_ids(self) -> List[int]:
        # æ”¶é›†æ‰€æœ‰æˆåŠŸé¡µé¢çš„ä¹¦ç±ID
        all_novel_ids = set()
        for page_result in self.success_items:
            novel_ids = page_result.get_novel_ids()
            # è¿‡æ»¤æ‰ç©ºå€¼å’Œæ— æ•ˆID
            valid_ids = [nid for nid in novel_ids if nid and str(nid).strip() and str(nid) != '0']
            all_novel_ids.update(valid_ids)
        return list(all_novel_ids)


@dataclass
class NovelsResult(BaseResult[NovelPageParser]):
    """
    å¤šä¹¦ç±é¡µé¢çˆ¬å–ç»“æœ
    """

    def to_dict(self) -> Dict[str, Any]:
        return {
            "total_novels_num": self.total_num,
            "failed_novels": self.failed_ids
        }


crawler_config = get_settings().crawler
book_service = BookService()
ranking_service = RankingService()
crawl_task = get_crawl_task()


def create_retry_decorator():
    """
    åˆ›å»ºåŸºäºé…ç½®çš„æ™ºèƒ½é‡è¯•è£…é¥°å™¨ - ä¼˜åŒ–å¹¶å‘æ§åˆ¶ + 503æš‚åœæœºåˆ¶
    
    ç‰¹æ€§ï¼š
    - ä½¿ç”¨é…ç½®æ–‡ä»¶å‚æ•°
    - æœ€å¤§é‡è¯•æ—¶é—´é™åˆ¶
    - æŒ‡æ•°é€€é¿ç­–ç•¥ + éšæœºå»¶è¿Ÿï¼Œé¿å…é‡è¯•é£æš´
    - 503é”™è¯¯è‡ªåŠ¨è§¦å‘å…¨å±€æš‚åœæœºåˆ¶
    - å¢å¼ºè°ƒè¯•æ—¥å¿—
    """

    retry_attempts = max(3, crawler_config.retry_times)  # é™ä½é‡è¯•æ¬¡æ•°åˆ°3æ¬¡
    max_time = max(60.0, crawler_config.max_retry_time)  # è°ƒæ•´ä¸º60ç§’ï¼Œæ›´åˆç†

    def should_retry(exception):
        """å¢å¼ºçš„é‡è¯•æ¡ä»¶åˆ¤æ–­ - åŒ…å«503é”™è¯¯å¤„ç†"""
        retry_types = (ValueError, KeyError,
                       httpx.RequestError, httpx.HTTPStatusError,
                       httpx.TimeoutException, json.JSONDecodeError)

        should_retry_result = isinstance(exception, retry_types)

        # ç‰¹æ®Šå¤„ç†503é”™è¯¯ï¼šè§¦å‘å…¨å±€æš‚åœ
        if isinstance(exception, httpx.HTTPStatusError) and exception.response.status_code == 503:
            logger.warning(f"ğŸš¨ é‡è¯•è£…é¥°å™¨æ£€æµ‹åˆ°503é”™è¯¯ï¼Œå³å°†è§¦å‘å…¨å±€æš‚åœ: {str(exception)}")
            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡æ¥è®¾ç½®æš‚åœçŠ¶æ€ï¼ˆåœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­ï¼‰
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # å¦‚æœäº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œï¼Œåˆ›å»ºä»»åŠ¡
                    loop.create_task(mark_server_need_pause())
                else:
                    # å¦‚æœæ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œç›´æ¥è¿è¡Œ
                    asyncio.run(mark_server_need_pause())
            except Exception as e:
                logger.error(f"è®¾ç½®503æš‚åœçŠ¶æ€å¤±è´¥: {e}")

        logger.info(
            f"é‡è¯•åˆ¤æ–­ - å¼‚å¸¸ç±»å‹: {type(exception).__name__}, å¼‚å¸¸æ¶ˆæ¯: {str(exception)}, æ˜¯å¦é‡è¯•: {should_retry_result}")
        return should_retry_result

    return retry(
        # åœæ­¢æ¡ä»¶ï¼šè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° OR è¶…è¿‡æœ€å¤§é‡è¯•æ—¶é—´
        stop=stop_after_attempt(retry_attempts) | stop_after_delay(max_time),
        # ç­‰å¾…ç­–ç•¥ï¼šæŒ‡æ•°é€€é¿ + éšæœºå»¶è¿Ÿï¼Œé˜²æ­¢é‡è¯•é£æš´
        wait=wait_exponential(
            multiplier=1.5,  # æ›´æ¸©å’Œçš„å€æ•°ï¼Œé¿å…ç­‰å¾…æ—¶é—´è¿‡é•¿
            min=3.0,  # å¢åŠ åŸºç¡€ç­‰å¾…æ—¶é—´åˆ°3ç§’
            max=20.0  # å¢åŠ å•æ¬¡ç­‰å¾…ä¸Šé™åˆ°20ç§’
        ),
        # é‡è¯•æ¡ä»¶ï¼šä½¿ç”¨è‡ªå®šä¹‰åˆ¤æ–­å‡½æ•°
        retry=retry_if_exception(should_retry),
        reraise=True
    )


class CrawlFlow:
    """
    ç»Ÿä¸€å¹¶å‘çˆ¬å–æµç¨‹ç®¡ç†å™¨ - ä¸¤é˜¶æ®µå¤„ç†æ¶æ„
    
    é˜¶æ®µ 1: è·å–æ‰€æœ‰é¡µé¢å†…å®¹
    é˜¶æ®µ 2: è·å–æ‰€æœ‰ä¹¦ç±å†…å®¹  
    å…¨å±€ç»Ÿä¸€å¹¶å‘æ§åˆ¶
    """

    def __init__(self) -> None:
        """
        åˆå§‹åŒ–çˆ¬å–æµç¨‹ç®¡ç†å™¨ - åŒ…å«503æš‚åœæœºåˆ¶
        """
        # æ¯ä¸ªå®ä¾‹åˆ›å»ºç‹¬ç«‹çš„HTTPå®¢æˆ·ç«¯å’Œå¹¶å‘æ§åˆ¶
        self.client = HttpClient()
        # è¿›ä¸€æ­¥é™ä½å¹¶å‘æ•°ï¼Œé¿å…503é”™è¯¯
        actual_concurrent = min(3, crawler_config.max_concurrent_requests)
        self.request_semaphore = asyncio.Semaphore(actual_concurrent)
        logger.info(f"åˆå§‹åŒ–çˆ¬è™«å®ä¾‹ï¼Œå®é™…å¹¶å‘æ•°: {actual_concurrent}ï¼ŒåŒ…å«503æš‚åœæœºåˆ¶")

    async def execute_crawl_task(self, page_ids: List[str]) -> Dict[str, Any]:
        """
        æ‰§è¡Œç»Ÿä¸€å¹¶å‘çˆ¬å–ä»»åŠ¡ - ä¸¤é˜¶æ®µå¤„ç†æ¶æ„
        
        Args:
            page_ids: é¡µé¢IDæˆ–é¡µé¢IDåˆ—è¡¨
            
        Returns:
            çˆ¬å–ç»“æœ
        """
        page_tasks = crawl_task.get_tasks_by_words(page_ids)

        start_time = time.time()
        logger.info(f"å¼€å§‹ç»Ÿä¸€å¹¶å‘çˆ¬å– {len(page_ids)} ä¸ªé¡µé¢: {page_ids}")
        try:
            # é˜¶æ®µ 1: è·å–æ‰€æœ‰é¡µé¢å†…å®¹
            page_data = await self._fetch_pages(page_tasks)

            # é˜¶æ®µ 2: è·å–æ‰€æœ‰ä¹¦ç±å†…å®¹
            book_data = await self._fetch_books(page_data)

            # é˜¶æ®µ 3: ä¿å­˜æ•°æ®
            save_results = await self._save_data(page_data, book_data)

            execution_time = time.time() - start_time
            logger.info(f"ç»Ÿä¸€å¹¶å‘çˆ¬å–æ€»è€—æ—¶ {execution_time:.2f}s")

            # ä½¿ç”¨ç±»å‹å®‰å…¨çš„ç»“æœæ„å»º
            return {
                "success": True,
                "page_results": page_data.to_dict(),
                "book_results": book_data.to_dict(),
                "store_results": save_results,
                "execution_time": execution_time
            }

        except Exception as e:
            logger.error(f"çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "exception": e
            }

    async def _fetch_pages(self, page_tasks: List[PageTask]) -> PagesResult:
        """
        é˜¶æ®µ 1: å¹¶å‘è·å–æ‰€æœ‰é¡µé¢å†…å®¹
        
        Args:
            page_tasks: é¡µé¢IDåˆ—è¡¨
            
        Returns:
            ç±»å‹å®‰å…¨çš„é¡µé¢ç»“æœ
        """
        logger.info(f"é˜¶æ®µ 1: å¼€å§‹è·å– {len(page_tasks)} ä¸ªé¡µé¢å†…å®¹")

        # åˆ›å»ºæ‰€æœ‰é¡µé¢çš„è·å–ä»»åŠ¡
        tasks = [self._fetch_and_parse_page(t) for t in page_tasks]
        pages = await asyncio.gather(*tasks, return_exceptions=True)

        # ä½¿ç”¨ç±»å‹å®‰å…¨çš„ç»“æœç±»
        pages_result = PagesResult()

        for t, result in zip(page_tasks, pages):
            if isinstance(result, Exception):
                pages_result.failed_items[t.id] = result
            else:
                pages_result.success_items.append(result)

        logger.info(f"é˜¶æ®µ 1 å®Œæˆ: æˆåŠŸ {len(pages_result.success_items)}/{pages_result.total_num} ä¸ªé¡µé¢")
        return pages_result

    @create_retry_decorator()
    async def _fetch_and_parse_page(self, page_task: PageTask) -> PageParser:
        async with self.request_semaphore:
            page_content = await self.client.run(page_task.url)
            if not page_content or page_content.get("status") == "error":
                raise ValueError(f"é¡µé¢å†…å®¹è·å–å¤±è´¥: {page_content.get('error', 'æœªçŸ¥é”™è¯¯')}")

            # è§£ææ¦œå•ä¿¡æ¯
            page_parser = PageParser(page_content, page_id=page_task.id)
            logger.info(f"é¡µé¢{page_task.id}è·å–å®Œæˆ: è§£ææ¦œå• {len(page_parser.rankings)}ä¸ª")
            return page_parser

    async def _fetch_books(self, pages_result: PagesResult) -> NovelsResult:
        """
        é˜¶æ®µ 2: å¹¶å‘è·å–æ‰€æœ‰ä¹¦ç±å†…å®¹
        
        Args:
            pages_result: ç±»å‹å®‰å…¨çš„é¡µé¢ç»“æœ
            
        Returns:
            ç±»å‹å®‰å…¨çš„ä¹¦ç±ç»“æœ
        """
        # æ”¶é›†æ‰€æœ‰æˆåŠŸé¡µé¢çš„ä¹¦ç±ID
        all_novel_ids = pages_result.get_novel_ids()
        if not all_novel_ids:
            logger.info("é˜¶æ®µ 2: æ— æœ‰æ•ˆä¹¦ç±IDéœ€è¦è·å–")
            return NovelsResult()

        logger.info(f"é˜¶æ®µ 2: å¼€å§‹è·å– {len(all_novel_ids)} ä¸ªä¹¦ç±å†…å®¹")

        # å¹¶å‘è·å–æ‰€æœ‰ä¹¦ç±å†…å®¹
        book_tasks = [self._fetch_and_parse_book(novel_id) for novel_id in all_novel_ids]
        book_results = await asyncio.gather(*book_tasks, return_exceptions=True)

        # ä½¿ç”¨ç±»å‹å®‰å…¨çš„ç»“æœç±»
        books_result = NovelsResult()
        for novel_id, result in zip(all_novel_ids, book_results):
            if isinstance(result, Exception):
                books_result.failed_items[str(novel_id)] = result
                logger.error(f"ä¹¦ç± {novel_id} è·å–å¤±è´¥: {result}")
            else:
                books_result.success_items.append(result)

        logger.info(f"é˜¶æ®µ 2 å®Œæˆ: æˆåŠŸ {len(books_result.success_items)}/{books_result.total_num} ä¸ªä¹¦ç±")

        return books_result

    @create_retry_decorator()
    async def _fetch_and_parse_book(self, novel_id: int) -> NovelPageParser:
        """
        ä¹¦ç±è·å– - åŒ…å«503æš‚åœæ£€æŸ¥æœºåˆ¶

        :param novel_id: ä¹¦ç±ID
        :return: ä¹¦ç±å“åº”æ•°æ®
        """
        # åœ¨å¼€å§‹å‰æ£€æŸ¥æš‚åœçŠ¶æ€ï¼ˆç¬¬ä¸€å±‚ä¿æŠ¤ï¼‰
        await check_and_pause_if_needed()
        
        # # æ·»åŠ éšæœºå»¶è¿Ÿæ¨¡æ‹Ÿäººç±»è¡Œä¸º
        # delay = random.uniform(0.5, 2.0)  # 0.5-2ç§’éšæœºå»¶è¿Ÿ
        # await asyncio.sleep(delay)
        
        logger.info(f"å¼€å§‹è·å–ä¹¦ç± {novel_id}ï¼Œé‡è¯•è£…é¥°å™¨å·²ç”Ÿæ•ˆ")
        async with self.request_semaphore:
            # åœ¨è·å–ä¿¡å·é‡åï¼Œå†æ¬¡æ£€æŸ¥æš‚åœçŠ¶æ€ï¼ˆåŒé‡ä¿æŠ¤ï¼‰
            await check_and_pause_if_needed()
            
            # å‚æ•°éªŒè¯
            if not novel_id:
                raise ValueError(f"Invalid novel_id parameter: '{novel_id}'")
            
            book_url = crawl_task.build_novel_url(str(novel_id))
            logger.info(f"æ­£åœ¨è¯·æ±‚ä¹¦ç±: {book_url}")
            result = await self.client.run(book_url)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ä¹¦ç±æ•°æ®ï¼ˆæ™‹æ±ŸAPIè¿”å›åŒ…å«novelIdçš„JSONæ•°æ®ï¼‰
            if not result.get("novelId"):
                raise KeyError(f"Invalid book data: missing novelId in response")

            novel_parser = NovelPageParser(result)
            return novel_parser

    async def _save_data(self, pages_result: PagesResult, novels_result: NovelsResult) -> Dict[str, int] | Exception:
        """
        é˜¶æ®µ 3: ä¿å­˜æ‰€æœ‰æ•°æ® - å®¹é”™ä¿å­˜æœºåˆ¶
        
        Args:
            pages_result: ç±»å‹å®‰å…¨çš„é¡µé¢ç»“æœ
            novels_result: ç±»å‹å®‰å…¨çš„ä¹¦ç±ç»“æœ
            
        Returns:
            ä¿å­˜ç»“æœç»Ÿè®¡
        """
        logger.info("é˜¶æ®µ 3: å¼€å§‹ä¿å­˜æ‰€æœ‰æ•°æ®")

        # æ”¶é›†æ‰€æœ‰æ¦œå•æ•°æ®
        all_rankings = pages_result.rankings
        books = novels_result.success_items

        ranking_snapshots_num = 0
        books_snapshots_num = 0

        # åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯å¹¶ä¿å­˜æ•°æ®
        db = SessionLocal()
        try:
            # ä½¿ç”¨ç°æœ‰çš„Serviceæ–¹æ³•ä¿å­˜æ•°æ®
            if all_rankings:
                _, ranking_snapshots_num = self.save_ranking_parsers(all_rankings, db)
                logger.info(f"ä¿å­˜äº† {len(all_rankings)} ä¸ªæ¦œå•ï¼Œ{ranking_snapshots_num} ä¸ªæ¦œå•å¿«ç…§")
            else:
                logger.info("æ²¡æœ‰æ¦œå•æ•°æ®éœ€è¦ä¿å­˜")
            if books:
                books_snapshots_num = self.save_novel_parsers(books, db)
                logger.info(f"ä¿å­˜äº† {len(books)} ä¸ªä¹¦ç±ï¼Œ{books_snapshots_num} ä¸ªä¹¦ç±å¿«ç…§")
            else:
                logger.info("æ²¡æœ‰ä¹¦ç±æ•°æ®éœ€è¦ä¿å­˜")
            db.commit()

            # æ›´å‡†ç¡®çš„å®Œæˆæ—¥å¿—
            total_saved = len(all_rankings) + len(books) + ranking_snapshots_num + books_snapshots_num
            if total_saved > 0:
                logger.info(f"é˜¶æ®µ 3 å®Œæˆ: æˆåŠŸä¿å­˜ {total_saved} æ¡æ•°æ®è®°å½•")
            else:
                logger.warning("é˜¶æ®µ 3 å®Œæˆ: æ²¡æœ‰æ•°æ®è¢«ä¿å­˜åˆ°æ•°æ®åº“")

            return {
                "rankings": len(all_rankings),
                "ranking_snapshots": ranking_snapshots_num,
                "books": len(books),
                "books_snapshots": books_snapshots_num,
            }
        except Exception as db_error:
            db.rollback()
            logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {db_error}")
            return db_error
        finally:
            db.close()

    @staticmethod
    def save_ranking_parsers(rankings: List[RankingParser], db: Session) -> Tuple[int, int]:
        """
        ä¿å­˜ä»æ¦œå•ç½‘é¡µä¸­çˆ¬å–çš„æ¦œå•è®°å½•ã€æ¦œå•ä¸­çš„ä¹¦ç±è®°å½•ã€æ¦œå•å¿«ç…§è®°å½•
        :param rankings:
        :param db:
        :return: ä¿å­˜çš„æ¦œå•æ•°é‡ï¼Œä¿å­˜çš„æ¦œå•å¿«ç…§æ•°é‡
        """
        stored_ranking_snapshots = 0
        for ranking in rankings:
            # ä¿å­˜æˆ–æ›´æ–°æ¦œå•ä¿¡æ¯
            rank_record = ranking_service.create_or_update_ranking(
                db, ranking.ranking_info
            )
            ranking_snapshots = []
            batch_id = generate_batch_id()
            stored_ranking_snapshots += len(ranking.book_snapshots)
            for book in ranking.book_snapshots:
                try:
                    # ä¿å­˜ä¹¦ç±
                    book_record = book_service.create_or_update_book(db, book)
                    # åˆ›å»ºæ¦œå•å¿«ç…§è®°å½•
                    snapshot_data = {
                        "ranking_id": rank_record.id,
                        "book_id": book_record.id,
                        "batch_id": batch_id,
                        **book
                    }
                    ranking_snapshots.append(snapshot_data)
                except Exception as e:
                    # å›æ»šå½“å‰äº‹åŠ¡ï¼Œé‡æ–°å¼€å§‹
                    try:
                        db.rollback()
                    except Exception:
                        pass  # å¿½ç•¥å›æ»šå¤±è´¥
                    logger.error(f"ä¹¦ç±ä¿å­˜å¼‚å¸¸ï¼Œè·³è¿‡è¯¥è®°å½•: {book.get('novel_id', 'unknown')}, é”™è¯¯: {e}")
                    continue

            # æ‰¹é‡ä¿å­˜æ¦œå•å¿«ç…§
            if ranking_snapshots:
                ranking_service.batch_create_ranking_snapshots(
                    db, ranking_snapshots, batch_id
                )
        return len(rankings), stored_ranking_snapshots

    @staticmethod
    def save_novel_parsers(books: List[NovelPageParser], db: Session) -> int:
        """
        ä¿å­˜ä¹¦ç±å¿«ç…§
        :param books:
        :param db:
        :return:
        """
        # ä¿å­˜ä¹¦ç±å¿«ç…§
        book_snapshots = []
        for book_data in books:
            try:
                # ä¿å­˜æˆ–æ›´æ–°ä¹¦ç±åŸºæœ¬ä¿¡æ¯
                book_info = book_data.book_detail
                book_record = book_service.create_or_update_book(db, book_info)

                if book_record is None:
                    logger.warning(f"ä¹¦ç±ä¿å­˜å¤±è´¥ï¼Œè·³è¿‡è¯¥è®°å½•: {book_info.get('novel_id', 'unknown')}")
                    continue

                # åˆ›å»ºä¹¦ç±å¿«ç…§è®°å½•
                snapshot_data = {
                    "book_id": book_record.id,
                    **book_info
                }
                book_snapshots.append(snapshot_data)
            except Exception as e:
                # å›æ»šå½“å‰äº‹åŠ¡ï¼Œé‡æ–°å¼€å§‹
                try:
                    db.rollback()
                except Exception:
                    pass  # å¿½ç•¥å›æ»šå¤±è´¥
                logger.error(f"ä¹¦ç±ä¿å­˜å¼‚å¸¸ï¼Œè·³è¿‡è¯¥è®°å½•: {book_info.get('novel_id', 'unknown')}, é”™è¯¯: {e}")
                continue
        # æ‰¹é‡ä¿å­˜ä¹¦ç±å¿«ç…§
        if book_snapshots:
            book_service.batch_create_book_snapshots(db, book_snapshots)
        return len(book_snapshots)

    async def close(self) -> None:
        """å…³é—­èµ„æº"""
        await self.client.close()


# å…¨å±€çˆ¬è™«å®ä¾‹ç®¡ç†
_craw_flow: CrawlFlow | None = None


def get_crawl_flow() -> CrawlFlow:
    """è·å–çˆ¬è™«ç¨‹åºå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _craw_flow
    if _craw_flow is None:
        _craw_flow = CrawlFlow()
    return _craw_flow


def crawl_task_wrapper(page_ids: List[str]) -> Dict[str, Any]:
    """
    APSchedulerä»»åŠ¡åŒ…è£…å‡½æ•° - åœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
    
    ä¿®å¤Event loopé—®é¢˜ï¼šæ¯æ¬¡ä»»åŠ¡æ‰§è¡Œæ—¶åˆ›å»ºæ–°çš„CrawlFlowå®ä¾‹
    
    Args:
        page_ids: é¡µé¢IDåˆ—è¡¨
        
    Returns:
        çˆ¬å–ç»“æœå­—å…¸
    """
    import asyncio

    async def async_crawl_task():
        # æ¯æ¬¡ä»»åŠ¡æ‰§è¡Œæ—¶åˆ›å»ºæ–°çš„CrawlFlowå®ä¾‹ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª
        crawl_flow = CrawlFlow()
        try:
            result = await crawl_flow.execute_crawl_task(page_ids)
            return result
        finally:
            # ç¡®ä¿èµ„æºæ¸…ç†
            await crawl_flow.close()

    try:
        # åœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        result = asyncio.run(async_crawl_task())

        logger.info(f"çˆ¬å–ä»»åŠ¡åŒ…è£…å‡½æ•°æ‰§è¡Œå®Œæˆï¼šæˆåŠŸ={result.get('success', False)}")
        return result

    except Exception as e:
        error_msg = f"çˆ¬å–ä»»åŠ¡åŒ…è£…å‡½æ•°æ‰§è¡Œå¤±è´¥: {str(e)}"
        logger.error(error_msg)
        return {
            "success": False,
            "error": error_msg,
            "exception_type": type(e).__name__
        }


if __name__ == '__main__':
    import asyncio


    async def debug_book_fetch():
        c = get_crawl_flow()
        result = await c._fetch_and_parse_book("3980442")
        print(f"è°ƒè¯•ç»“æœ: {result}")

        # å¦‚æœè·å–æˆåŠŸï¼Œæ˜¾ç¤ºè§£æçš„ä¹¦ç±ä¿¡æ¯
        if hasattr(result, 'book_detail'):
            print(f"ä¹¦ç±è¯¦æƒ…: {result.book_detail}")

        await c.close()


    async def debug_task():
        from app.database.connection import ensure_db
        ensure_db()
        c = get_crawl_flow()
        result = await c.execute_crawl_task(["all"])
        print(result)


    asyncio.run(debug_task())
